
@inproceedings{10.5555/777092.777184,
  title = {{{FastSLAM}}: {{A}} Factored Solution to the Simultaneous Localization and Mapping Problem},
  booktitle = {Eighteenth National Conference on Artificial Intelligence},
  author = {Montemerlo, Michael and Thrun, Sebastian and Koller, Daphne and Wegbreit, Ben},
  year = {2002},
  pages = {593--598},
  publisher = {{American Association for Artificial Intelligence}},
  address = {{USA}},
  abstract = {The ability to simultaneously localize a robot and accurately map its surroundings is considered by many to be a key prerequisite of truly autonomous robots. However, few approaches to this problem scale up to handle the very large number of landmarks present in real environments. Kalman filter-based algorithms, for example, require time quadratic in the number of landmarks to incorporate each sensor observation. This paper presents FastSLAM, an algorithm that recursively estimates the full posterior distribution over robot pose and landmark locations, yet scales logarithmically with the number of landmarks in the map. This algorithm is based on an exact factorization of the posterior into a product of conditional landmark distributions and a distribution over robot paths. The algorithm has been run successfully on as many as 50,000 landmarks, environments far beyond the reach of previous approaches. Experimental results demonstrate the advantages and limitations of the FastSLAM algorithm on both simulated and real-world data.},
  isbn = {0-262-51129-0}
}

@misc{200314338TartanAir,
  title = {[2003.14338] {{TartanAir}}: {{A Dataset}} to {{Push}} the {{Limits}} of {{Visual SLAM}}},
  file = {/Users/anselm/Zotero/storage/UTJIGEXN/2003.html},
  howpublished = {https://arxiv.org/abs/2003.14338}
}

@book{adamsLecturesExceptionalLie1996,
  title = {Lectures on Exceptional {{Lie}} Groups},
  author = {Adams, J. Frank and Mahmud, Zafer and Mimura, M.},
  year = {1996},
  publisher = {{University of Chicago Press}},
  address = {{Chicago}},
  isbn = {978-0-226-00526-3 978-0-226-00527-0},
  keywords = {Lie groups},
  lccn = {QA387 .A33 1996},
  series = {Chicago Lectures in Mathematics Series}
}

@article{agarwalBuildingRomeDay2011,
  title = {Building {{Rome}} in a Day},
  author = {Agarwal, Sameer and Furukawa, Yasutaka and Snavely, Noah and Simon, Ian and Curless, Brian and Seitz, Steven M. and Szeliski, Richard},
  year = {2011},
  month = oct,
  volume = {54},
  pages = {105--112},
  issn = {0001-0782, 1557-7317},
  doi = {10.1145/2001269.2001293},
  file = {/Users/anselm/Zotero/storage/PK9XQWT6/rome_paper.pdf},
  journal = {Communications of the ACM},
  language = {en},
  number = {10}
}

@book{ahujaNetworkFlowsTheory1993,
  title = {Network Flows: Theory, Algorithms, and Applications},
  shorttitle = {Network Flows},
  author = {Ahuja, Ravindra K. and Magnanti, Thomas L. and Orlin, James B.},
  year = {1993},
  publisher = {{Prentice Hall}},
  address = {{Englewood Cliffs, N.J}},
  isbn = {978-0-13-617549-0},
  keywords = {Mathematical optimization,Network analysis (Planning)},
  lccn = {T57.85 .A37 1993}
}

@inproceedings{alcantarillaFastExplicitDiffusion2013,
  title = {Fast {{Explicit Diffusion}} for {{Accelerated Features}} in {{Nonlinear Scale Spaces}}},
  booktitle = {Procedings of the {{British Machine Vision Conference}} 2013},
  author = {Alcantarilla, Pablo and Nuevo, Jesus and Bartoli, Adrien},
  year = {2013},
  pages = {13.1-13.11},
  publisher = {{British Machine Vision Association}},
  address = {{Bristol}},
  doi = {10.5244/C.27.13},
  abstract = {We propose a novel and fast multiscale feature detection and description approach that exploits the benefits of nonlinear scale spaces. Previous attempts to detect and describe features in nonlinear scale spaces are highly time consuming due to the computational burden of creating the nonlinear scale space. In this paper we propose to use recent numerical schemes called Fast Explicit Diffusion (FED) embedded in a pyramidal framework to dramatically speed-up feature detection in nonlinear scale spaces. In addition, we introduce a Modified-Local Difference Binary (M-LDB) descriptor that is highly efficient, exploits gradient information from the nonlinear scale space, is scale and rotation invariant and has low storage requirements. We present an extensive evaluation that shows the excellent compromise between speed and performance of our approach compared to state-of-the-art methods such as BRISK, ORB, SURF, SIFT and KAZE.},
  file = {/Users/anselm/Zotero/storage/8FBP582P/Alcantarilla et al. - 2013 - Fast Explicit Diffusion for Accelerated Features i.pdf},
  isbn = {978-1-901725-49-0},
  language = {en}
}

@article{badrinarayananSegNetDeepConvolutional2017,
  title = {{{SegNet}}: {{A Deep Convolutional Encoder}}-{{Decoder Architecture}} for {{Image Segmentation}}},
  shorttitle = {{{SegNet}}},
  author = {Badrinarayanan, Vijay and Kendall, Alex and Cipolla, Roberto},
  year = {2017},
  month = dec,
  volume = {39},
  pages = {2481--2495},
  issn = {0162-8828, 2160-9292, 1939-3539},
  doi = {10.1109/TPAMI.2016.2644615},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number = {12}
}

@article{baileySimultaneousLocalisationMapping,
  title = {Simultaneous {{Localisation}} and {{Mapping}} ({{SLAM}}): {{Part II State}} of the {{Art}}},
  author = {Bailey, Tim and {Durrant-Whyte}, Hugh},
  pages = {10},
  abstract = {This tutorial provides an introduction to the Simultaneous Localisation and Mapping (SLAM) method and the extensive research on SLAM that has been undertaken. Part I of this tutorial described the essential SLAM problem. Part II of this tutorial (this paper) is concerned with recent advances in computational methods and in new formulations of the SLAM problem for large scale and complex environments.},
  file = {/Users/anselm/Zotero/storage/BTTPU37X/10.1.1.110.4868.pdf;/Users/anselm/Zotero/storage/VM294RXM/Bailey and Durrant-Whyte - Simultaneous Localisation and Mapping (SLAM) Part.pdf},
  language = {en}
}

@article{bankAutoencoders2020,
  title = {Autoencoders},
  author = {Bank, Dor and Koenigstein, Noam and Giryes, Raja},
  year = {2020},
  month = mar,
  abstract = {An autoencoder is a specific type of a neural network, which is mainlydesigned to encode the input into a compressed and meaningful representation, andthen decode it back such that the reconstructed input is similar as possible to theoriginal one. This chapter surveys the different types of autoencoders that are mainlyused today. It also describes various applications and use-cases of autoencoders.},
  archiveprefix = {arXiv},
  eprint = {2003.05991},
  eprinttype = {arxiv},
  file = {/Users/anselm/Zotero/storage/C5P5RR7W/Bank et al. - 2020 - Autoencoders.pdf;/Users/anselm/Zotero/storage/RYTGK84A/2003.html},
  journal = {arXiv:2003.05991 [cs, stat]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  primaryclass = {cs, stat}
}

@book{barfootStateEstimationRobotics2017,
  title = {State {{Estimation}} for {{Robotics}}},
  author = {Barfoot, Timothy D.},
  year = {2017},
  publisher = {{Cambridge University Press}},
  address = {{Cambridge}},
  doi = {10.1017/9781316671528},
  file = {/Users/anselm/Zotero/storage/AB24X3KB/Barfoot - 2017 - State Estimation for Robotics.pdf},
  isbn = {978-1-316-67152-8},
  language = {en}
}

@article{barsanRobustDenseMapping2018,
  title = {Robust {{Dense Mapping}} for {{Large}}-{{Scale Dynamic Environments}}},
  author = {B{\^a}rsan, Ioan Andrei and Liu, Peidong and Pollefeys, Marc and Geiger, Andreas},
  year = {2018},
  month = may,
  pages = {7510--7517},
  doi = {10.1109/ICRA.2018.8462974},
  abstract = {We present a stereo-based dense mapping algorithm for large-scale dynamic urban environments. In contrast to other existing methods, we simultaneously reconstruct the static background, the moving objects, and the potentially moving but currently stationary objects separately, which is desirable for high-level mobile robotic tasks such as path planning in crowded environments. We use both instance-aware semantic segmentation and sparse scene flow to classify objects as either background, moving, or potentially moving, thereby ensuring that the system is able to model objects with the potential to transition from static to dynamic, such as parked cars. Given camera poses estimated from visual odometry, both the background and the (potentially) moving objects are reconstructed separately by fusing the depth maps computed from the stereo input. In addition to visual odometry, sparse scene flow is also used to estimate the 3D motions of the detected moving objects, in order to reconstruct them accurately. A map pruning technique is further developed to improve reconstruction accuracy and reduce memory consumption, leading to increased scalability. We evaluate our system thoroughly on the well-known KITTI dataset. Our system is capable of running on a PC at approximately 2.5Hz, with the primary bottleneck being the instance-aware semantic segmentation, which is a limitation we hope to address in future work. The source code is available from the project website (http://andreibarsan.github.io/dynslam).},
  archiveprefix = {arXiv},
  eprint = {1905.02781},
  eprinttype = {arxiv},
  file = {/Users/anselm/Zotero/storage/IL5FAD42/BÃ¢rsan et al. - 2018 - Robust Dense Mapping for Large-Scale Dynamic Envir.pdf;/Users/anselm/Zotero/storage/IVSP5QT9/1905.html},
  journal = {2018 IEEE International Conference on Robotics and Automation (ICRA)},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Robotics}
}

@article{baserFANTrack3DMultiObject2019,
  title = {{{FANTrack}}: {{3D Multi}}-{{Object Tracking}} with {{Feature Association Network}}},
  shorttitle = {{{FANTrack}}},
  author = {Baser, Erkan and Balasubramanian, Venkateshwaran and Bhattacharyya, Prarthana and Czarnecki, Krzysztof},
  year = {2019},
  month = may,
  abstract = {We propose a data-driven approach to online multi-object tracking (MOT) that uses a convolutional neural network (CNN) for data association in a tracking-by-detection framework. The problem of multi-target tracking aims to assign noisy detections to a-priori unknown and time-varying number of tracked objects across a sequence of frames. A majority of the existing solutions focus on either tediously designing cost functions or formulating the task of data association as a complex optimization problem that can be solved effectively. Instead, we exploit the power of deep learning to formulate the data association problem as inference in a CNN. To this end, we propose to learn a similarity function that combines cues from both image and spatial features of objects. Our solution learns to perform global assignments in 3D purely from data, handles noisy detections and a varying number of targets, and is easy to train. We evaluate our approach on the challenging KITTI dataset and show competitive results. Our code is available at https://git.uwaterloo.ca/wise-lab/fantrack.},
  archiveprefix = {arXiv},
  eprint = {1905.02843},
  eprinttype = {arxiv},
  file = {/Users/anselm/Zotero/storage/KR7LSPSM/Baser et al. - 2019 - FANTrack 3D Multi-Object Tracking with Feature As.pdf;/Users/anselm/Zotero/storage/6VWJADNH/1905.html},
  journal = {arXiv:1905.02843 [cs]},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Robotics},
  primaryclass = {cs}
}

@incollection{baySURFSpeededRobust2006,
  title = {{{SURF}}: {{Speeded Up Robust Features}}},
  shorttitle = {{{SURF}}},
  booktitle = {Computer {{Vision}} \textendash{} {{ECCV}} 2006},
  author = {Bay, Herbert and Tuytelaars, Tinne and Van Gool, Luc},
  editor = {Leonardis, Ale{\v s} and Bischof, Horst and Pinz, Axel},
  year = {2006},
  volume = {3951},
  pages = {404--417},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/11744023_32},
  abstract = {In this paper, we present a novel scale- and rotation-invariant interest point detector and descriptor, coined SURF (Speeded Up Robust Features). It approximates or even outperforms previously proposed schemes with respect to repeatability, distinctiveness, and robustness, yet can be computed and compared much faster.},
  file = {/Users/anselm/Zotero/storage/7HYAQ7U7/Bay et al. - 2006 - SURF Speeded Up Robust Features.pdf},
  isbn = {978-3-540-33832-1 978-3-540-33833-8},
  language = {en}
}

@article{bernardinEvaluatingMultipleObject2008,
  title = {Evaluating {{Multiple Object Tracking Performance}}: {{The CLEAR MOT Metrics}}},
  shorttitle = {Evaluating {{Multiple Object Tracking Performance}}},
  author = {Bernardin, Keni and Stiefelhagen, Rainer},
  year = {2008},
  volume = {2008},
  pages = {1--10},
  issn = {1687-5176, 1687-5281},
  doi = {10.1155/2008/246309},
  file = {/Users/anselm/Zotero/storage/T4QP8WNY/Bernardin and Stiefelhagen - 2008 - Evaluating Multiple Object Tracking Performance T.pdf},
  journal = {EURASIP Journal on Image and Video Processing},
  language = {en}
}

@article{bescosDynaSLAMTrackingMapping2018,
  title = {{{DynaSLAM}}: {{Tracking}}, {{Mapping}} and {{Inpainting}} in {{Dynamic Scenes}}},
  shorttitle = {{{DynaSLAM}}},
  author = {Bescos, Berta and F{\'a}cil, Jos{\'e} M. and Civera, Javier and Neira, Jos{\'e}},
  year = {2018},
  month = aug,
  doi = {10.1109/LRA.2018.2860039},
  abstract = {The assumption of scene rigidity is typical in SLAM algorithms. Such a strong assumption limits the use of most visual SLAM systems in populated real-world environments, which are the target of several relevant applications like service robotics or autonomous vehicles. In this paper we present DynaSLAM, a visual SLAM system that, building over ORB-SLAM2 [1], adds the capabilities of dynamic object detection and background inpainting. DynaSLAM is robust in dynamic scenarios for monocular, stereo and RGB-D configurations. We are capable of detecting the moving objects either by multi-view geometry, deep learning or both. Having a static map of the scene allows inpainting the frame background that has been occluded by such dynamic objects. We evaluate our system in public monocular, stereo and RGB-D datasets. We study the impact of several accuracy/speed trade-offs to assess the limits of the proposed methodology. DynaSLAM outperforms the accuracy of standard visual SLAM baselines in highly dynamic scenarios. And it also estimates a map of the static parts of the scene, which is a must for long-term applications in real-world environments.},
  archiveprefix = {arXiv},
  eprint = {1806.05620},
  eprinttype = {arxiv},
  file = {/Users/anselm/Zotero/storage/CXGIN8ZD/Bescos et al. - 2018 - DynaSLAM Tracking, Mapping and Inpainting in Dyna.pdf;/Users/anselm/Zotero/storage/FEQLWAWE/1806.html},
  journal = {arXiv:1806.05620 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  primaryclass = {cs}
}

@article{bewleySimpleOnlineRealtime2016,
  title = {Simple {{Online}} and {{Realtime Tracking}}},
  author = {Bewley, Alex and Ge, Zongyuan and Ott, Lionel and Ramos, Fabio and Upcroft, Ben},
  year = {2016},
  month = sep,
  pages = {3464--3468},
  doi = {10.1109/ICIP.2016.7533003},
  abstract = {This paper explores a pragmatic approach to multiple object tracking where the main focus is to associate objects efficiently for online and realtime applications. To this end, detection quality is identified as a key factor influencing tracking performance, where changing the detector can improve tracking by up to 18.9\%. Despite only using a rudimentary combination of familiar techniques such as the Kalman Filter and Hungarian algorithm for the tracking components, this approach achieves an accuracy comparable to state-of-the-art online trackers. Furthermore, due to the simplicity of our tracking method, the tracker updates at a rate of 260 Hz which is over 20x faster than other state-of-the-art trackers.},
  archiveprefix = {arXiv},
  eprint = {1602.00763},
  eprinttype = {arxiv},
  file = {/Users/anselm/Zotero/storage/BLJRQ3KQ/Bewley et al. - 2016 - Simple Online and Realtime Tracking.pdf;/Users/anselm/Zotero/storage/EL6UEZLS/1602.html},
  journal = {2016 IEEE International Conference on Image Processing (ICIP)},
  keywords = {Computer Science - Computer Vision and Pattern Recognition}
}

@book{bishopPatternRecognitionMachine2006,
  title = {Pattern Recognition and Machine Learning},
  author = {Bishop, Christopher M.},
  year = {2006},
  publisher = {{Springer}},
  address = {{New York}},
  file = {/Users/anselm/Zotero/storage/GKH9YX49/Bishop - 2006 - Pattern recognition and machine learning.pdf},
  isbn = {978-0-387-31073-2},
  keywords = {Machine learning,Pattern perception},
  language = {en},
  lccn = {Q327 .B52 2006},
  series = {Information Science and Statistics}
}

@book{bjorckNumericalMethodsLeast1996,
  title = {Numerical Methods for Least Squares Problems},
  author = {Bj{\"o}rck, {\AA}ke},
  year = {1996},
  publisher = {{SIAM}},
  address = {{Philadelphia}},
  isbn = {978-0-89871-360-2},
  keywords = {Equations; Simultaneous,Least squares,Numerical solutions},
  lccn = {QA214 .B56 1996}
}

@book{bottemaTheoreticalKinematics1990,
  title = {Theoretical Kinematics},
  author = {Bottema, O. and Roth, Bernard},
  year = {1990},
  publisher = {{Dover Publications}},
  address = {{New York}},
  isbn = {978-0-486-66346-3},
  keywords = {Kinematics},
  lccn = {QA841 .B66 1990}
}

@book{brownIntroductionRandomSignals1997,
  title = {Introduction to Random Signals and Applied {{Kalman}} Filtering: With {{MATLAB}} Exercises and Solutions},
  shorttitle = {Introduction to Random Signals and Applied {{Kalman}} Filtering},
  author = {Brown, Robert Grover and Hwang, Patrick Y. C.},
  year = {1997},
  edition = {3rd ed},
  publisher = {{Wiley}},
  address = {{New York}},
  isbn = {978-0-471-12839-7},
  keywords = {Data processing,Kalman filtering,MATLAB,Random noise theory,Signal processing},
  lccn = {TK5102.9 .B75 1997}
}

@article{cadenaPresentFutureSimultaneous2016,
  title = {Past, {{Present}}, and {{Future}} of {{Simultaneous Localization And Mapping}}: {{Towards}} the {{Robust}}-{{Perception Age}}},
  shorttitle = {Past, {{Present}}, and {{Future}} of {{Simultaneous Localization And Mapping}}},
  author = {Cadena, Cesar and Carlone, Luca and Carrillo, Henry and Latif, Yasir and Scaramuzza, Davide and Neira, Jose and Reid, Ian and Leonard, John J.},
  year = {2016},
  month = dec,
  volume = {32},
  pages = {1309--1332},
  issn = {1552-3098, 1941-0468},
  doi = {10.1109/TRO.2016.2624754},
  abstract = {Simultaneous Localization and Mapping (SLAM)consists in the concurrent construction of a model of the environment (the map), and the estimation of the state of the robot moving within it. The SLAM community has made astonishing progress over the last 30 years, enabling large-scale real-world applications, and witnessing a steady transition of this technology to industry. We survey the current state of SLAM. We start by presenting what is now the de-facto standard formulation for SLAM. We then review related work, covering a broad set of topics including robustness and scalability in long-term mapping, metric and semantic representations for mapping, theoretical performance guarantees, active SLAM and exploration, and other new frontiers. This paper simultaneously serves as a position paper and tutorial to those who are users of SLAM. By looking at the published research with a critical eye, we delineate open challenges and new research issues, that still deserve careful scientific investigation. The paper also contains the authors' take on two questions that often animate discussions during robotics conferences: Do robots need SLAM? and Is SLAM solved?},
  archiveprefix = {arXiv},
  eprint = {1606.05830},
  eprinttype = {arxiv},
  file = {/Users/anselm/Zotero/storage/L575FVU8/Cadena et al. - 2016 - Past, Present, and Future of Simultaneous Localiza.pdf},
  journal = {IEEE Transactions on Robotics},
  keywords = {Computer Science - Robotics},
  language = {en},
  number = {6}
}

@article{caesarNuScenesMultimodalDataset2020,
  title = {{{nuScenes}}: {{A}} Multimodal Dataset for Autonomous Driving},
  shorttitle = {{{nuScenes}}},
  author = {Caesar, Holger and Bankiti, Varun and Lang, Alex H. and Vora, Sourabh and Liong, Venice Erin and Xu, Qiang and Krishnan, Anush and Pan, Yu and Baldan, Giancarlo and Beijbom, Oscar},
  year = {2020},
  month = may,
  abstract = {Robust detection and tracking of objects is crucial for the deployment of autonomous vehicle technology. Image based benchmark datasets have driven development in computer vision tasks such as object detection, tracking and segmentation of agents in the environment. Most autonomous vehicles, however, carry a combination of cameras and range sensors such as lidar and radar. As machine learning based methods for detection and tracking become more prevalent, there is a need to train and evaluate such methods on datasets containing range sensor data along with images. In this work we present nuTonomy scenes (nuScenes), the first dataset to carry the full autonomous vehicle sensor suite: 6 cameras, 5 radars and 1 lidar, all with full 360 degree field of view. nuScenes comprises 1000 scenes, each 20s long and fully annotated with 3D bounding boxes for 23 classes and 8 attributes. It has 7x as many annotations and 100x as many images as the pioneering KITTI dataset. We define novel 3D detection and tracking metrics. We also provide careful dataset analysis as well as baselines for lidar and image based detection and tracking. Data, development kit and more information are available online.},
  archiveprefix = {arXiv},
  eprint = {1903.11027},
  eprinttype = {arxiv},
  file = {/Users/anselm/Zotero/storage/JMU3YHT6/Caesar et al. - 2020 - nuScenes A multimodal dataset for autonomous driv.pdf;/Users/anselm/Zotero/storage/3LUGZFFP/1903.html},
  journal = {arXiv:1903.11027 [cs, stat]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Robotics,Statistics - Machine Learning},
  primaryclass = {cs, stat}
}

@article{caiUnifiedMultiscaleDeep2016,
  title = {A {{Unified Multi}}-Scale {{Deep Convolutional Neural Network}} for {{Fast Object Detection}}},
  author = {Cai, Zhaowei and Fan, Quanfu and Feris, Rogerio S. and Vasconcelos, Nuno},
  year = {2016},
  month = jul,
  abstract = {A unified deep neural network, denoted the multi-scale CNN (MS-CNN), is proposed for fast multi-scale object detection. The MS-CNN consists of a proposal sub-network and a detection sub-network. In the proposal sub-network, detection is performed at multiple output layers, so that receptive fields match objects of different scales. These complementary scale-specific detectors are combined to produce a strong multi-scale object detector. The unified network is learned end-to-end, by optimizing a multi-task loss. Feature upsampling by deconvolution is also explored, as an alternative to input upsampling, to reduce the memory and computation costs. State-of-the-art object detection performance, at up to 15 fps, is reported on datasets, such as KITTI and Caltech, containing a substantial number of small objects.},
  archiveprefix = {arXiv},
  eprint = {1607.07155},
  eprinttype = {arxiv},
  file = {/Users/anselm/Zotero/storage/TUVEN2XS/Cai et al. - 2016 - A Unified Multi-scale Deep Convolutional Neural Ne.pdf;/Users/anselm/Zotero/storage/JKG8ZPP8/1607.html},
  journal = {arXiv:1607.07155 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  primaryclass = {cs}
}

@article{cannyComputationalApproachEdge1986,
  title = {A {{Computational Approach}} to {{Edge Detection}}},
  author = {Canny, John},
  year = {1986},
  month = nov,
  volume = {PAMI-8},
  pages = {679--698},
  issn = {0162-8828},
  doi = {10.1109/TPAMI.1986.4767851},
  file = {/Users/anselm/Zotero/storage/DRMPQJZS/Canny - A Computational Approach to Edge Detection.pdf},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number = {6}
}

@article{chaabaneDEFTDetectionEmbeddings2021,
  title = {{{DEFT}}: {{Detection Embeddings}} for {{Tracking}}},
  shorttitle = {{{DEFT}}},
  author = {Chaabane, Mohamed and Zhang, Peter and Beveridge, J. Ross and O'Hara, Stephen},
  year = {2021},
  month = feb,
  abstract = {Most modern multiple object tracking (MOT) systems follow the tracking-by-detection paradigm, consisting of a detector followed by a method for associating detections into tracks. There is a long history in tracking of combining motion and appearance features to provide robustness to occlusions and other challenges, but typically this comes with the trade-off of a more complex and slower implementation. Recent successes on popular 2D tracking benchmarks indicate that top-scores can be achieved using a state-of-the-art detector and relatively simple associations relying on single-frame spatial offsets -- notably outperforming contemporary methods that leverage learned appearance features to help re-identify lost tracks. In this paper, we propose an efficient joint detection and tracking model named DEFT, or "Detection Embeddings for Tracking." Our approach relies on an appearance-based object matching network jointly-learned with an underlying object detection network. An LSTM is also added to capture motion constraints. DEFT has comparable accuracy and speed to the top methods on 2D online tracking leaderboards while having significant advantages in robustness when applied to more challenging tracking data. DEFT raises the bar on the nuScenes monocular 3D tracking challenge, more than doubling the performance of the previous top method. Code is publicly available.},
  archiveprefix = {arXiv},
  eprint = {2102.02267},
  eprinttype = {arxiv},
  file = {/Users/anselm/Zotero/storage/ZU3UHADR/Chaabane et al. - 2021 - DEFT Detection Embeddings for Tracking.pdf},
  journal = {arXiv:2102.02267 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  language = {en},
  primaryclass = {cs}
}

@article{chenObjectModellingRegistration1992,
  title = {Object Modelling by Registration of Multiple Range Images},
  author = {Chen, Yang and Medioni, G{\'e}rard},
  year = {1992},
  month = apr,
  volume = {10},
  pages = {145--155},
  issn = {02628856},
  doi = {10.1016/0262-8856(92)90066-C},
  journal = {Image and Vision Computing},
  language = {en},
  number = {3}
}

@inproceedings{chieh-chihwangOnlineSimultaneousLocalization2003,
  title = {Online Simultaneous Localization and Mapping with Detection and Tracking of Moving Objects: Theory and Results from a Ground Vehicle in Crowded Urban Areas},
  shorttitle = {Online Simultaneous Localization and Mapping with Detection and Tracking of Moving Objects},
  booktitle = {2003 {{IEEE International Conference}} on {{Robotics}} and {{Automation}} ({{Cat}}. {{No}}.{{03CH37422}})},
  author = {{Chieh-Chih Wang} and Thorpe, C. and Thrun, S.},
  year = {2003},
  volume = {1},
  pages = {842--849},
  publisher = {{IEEE}},
  address = {{Taipei, Taiwan}},
  doi = {10.1109/ROBOT.2003.1241698},
  abstract = {The simultaneous localization and mapping (SLAM) with detection and tracking of moving objects (DATMO) problem is not only to solve the SLAM problem in dynamic environments but also to detect and track these dynamic objects. In this paper, we derive the Bayesian formula of the SLAM with DATMO problem, which provides a solid basis for understanding and solving this problem. In addition, we provide a practical algorithm for performing DATMO from a moving platform equipped with range sensors. The probabilistic approach to solve the whole problem has been implemented with the Navlab11 vehicle. More than 100 miles of experiments in crowded urban areas indicated that SLAM with DATMO is indeed feasible.},
  file = {/Users/anselm/Zotero/storage/N3MN4JQ2/Chieh-Chih Wang et al. - 2003 - Online simultaneous localization and mapping with .pdf},
  isbn = {978-0-7803-7736-3},
  language = {en}
}

@article{chiuProbabilistic3DMultiObject2020,
  title = {Probabilistic {{3D Multi}}-{{Object Tracking}} for {{Autonomous Driving}}},
  author = {Chiu, Hsu-kuang and Prioletti, Antonio and Li, Jie and Bohg, Jeannette},
  year = {2020},
  month = jan,
  abstract = {3D multi-object tracking is a key module in autonomous driving applications that provides a reliable dynamic representation of the world to the planning module. In this paper, we present our on-line tracking method, which made the first place in the NuScenes Tracking Challenge, held at the AI Driving Olympics Workshop at NeurIPS 2019. Our method estimates the object states by adopting a Kalman Filter. We initialize the state covariance as well as the process and observation noise covariance with statistics from the training set. We also use the stochastic information from the Kalman Filter in the data association step by measuring the Mahalanobis distance between the predicted object states and current object detections. Our experimental results on the NuScenes validation and test set show that our method outperforms the AB3DMOT baseline method by a large margin in the Average Multi-Object Tracking Accuracy (AMOTA) metric.},
  archiveprefix = {arXiv},
  eprint = {2001.05673},
  eprinttype = {arxiv},
  file = {/Users/anselm/Zotero/storage/GPKYAKPE/Chiu et al. - 2020 - Probabilistic 3D Multi-Object Tracking for Autonom.pdf},
  journal = {arXiv:2001.05673 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Robotics},
  language = {en},
  primaryclass = {cs}
}

@article{colosiPlugandPlaySLAMUnified2020,
  title = {Plug-and-{{Play SLAM}}: {{A Unified SLAM Architecture}} for {{Modularity}} and {{Ease}} of {{Use}}},
  shorttitle = {Plug-and-{{Play SLAM}}},
  author = {Colosi, Mirco and Aloise, Irvin and Guadagnino, Tiziano and Schlegel, Dominik and Della Corte, Bartolomeo and Arras, Kai O. and Grisetti, Giorgio},
  year = {2020},
  month = apr,
  abstract = {Nowadays, Simultaneous Localization and Mapping (SLAM) is considered by the Robotics community to be a mature field. Currently, there are many open-source systems that are able to deliver fast and accurate estimation in typical real-world scenarios. Still, all these systems often provide an ad-hoc implementation that entailed to predefined sensor configurations. In this work, we tackle this issue, proposing a novel SLAM architecture specifically designed to address heterogeneous sensor arrangement and to standardize SLAM architecture. Thanks to its modularity and to specific design patterns, the presented framework is easy to extend, enhancing code reuse and efficiency. Finally, adopting our solution, we conducted comparative experiments for a variety of sensor configurations, showing competitive results that confirms stateof-the-art performance.},
  archiveprefix = {arXiv},
  eprint = {2003.00754},
  eprinttype = {arxiv},
  file = {/Users/anselm/Zotero/storage/EZDI753Z/1812.04244.pdf;/Users/anselm/Zotero/storage/FTXDHUSG/Colosi et al. - 2020 - Plug-and-Play SLAM A Unified SLAM Architecture fo.pdf},
  journal = {arXiv:2003.00754 [cs]},
  keywords = {Computer Science - Robotics},
  language = {en},
  primaryclass = {cs}
}

@article{cordtsCityscapesDatasetSemantic2016,
  title = {The {{Cityscapes Dataset}} for {{Semantic Urban Scene Understanding}}},
  author = {Cordts, Marius and Omran, Mohamed and Ramos, Sebastian and Rehfeld, Timo and Enzweiler, Markus and Benenson, Rodrigo and Franke, Uwe and Roth, Stefan and Schiele, Bernt},
  year = {2016},
  month = apr,
  abstract = {Visual understanding of complex urban street scenes is an enabling factor for a wide range of applications. Object detection has benefited enormously from large-scale datasets, especially in the context of deep learning. For semantic urban scene understanding, however, no current dataset adequately captures the complexity of real-world urban scenes. To address this, we introduce Cityscapes, a benchmark suite and large-scale dataset to train and test approaches for pixel-level and instance-level semantic labeling. Cityscapes is comprised of a large, diverse set of stereo video sequences recorded in streets from 50 different cities. 5000 of these images have high quality pixel-level annotations; 20000 additional images have coarse annotations to enable methods that leverage large volumes of weakly-labeled data. Crucially, our effort exceeds previous attempts in terms of dataset size, annotation richness, scene variability, and complexity. Our accompanying empirical study provides an in-depth analysis of the dataset characteristics, as well as a performance evaluation of several state-of-the-art approaches based on our benchmark.},
  archiveprefix = {arXiv},
  eprint = {1604.01685},
  eprinttype = {arxiv},
  file = {/Users/anselm/Zotero/storage/VUZ72BZL/Cordts et al. - 2016 - The Cityscapes Dataset for Semantic Urban Scene Un.pdf;/Users/anselm/Zotero/storage/FBNKLCYW/1604.html},
  journal = {arXiv:1604.01685 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  primaryclass = {cs}
}

@article{daiInstanceawareSemanticSegmentation2015,
  title = {Instance-Aware {{Semantic Segmentation}} via {{Multi}}-Task {{Network Cascades}}},
  author = {Dai, Jifeng and He, Kaiming and Sun, Jian},
  year = {2015},
  month = dec,
  abstract = {Semantic segmentation research has recently witnessed rapid progress, but many leading methods are unable to identify object instances. In this paper, we present Multi-task Network Cascades for instance-aware semantic segmentation. Our model consists of three networks, respectively differentiating instances, estimating masks, and categorizing objects. These networks form a cascaded structure, and are designed to share their convolutional features. We develop an algorithm for the nontrivial end-to-end training of this causal, cascaded structure. Our solution is a clean, single-step training framework and can be generalized to cascades that have more stages. We demonstrate state-of-the-art instance-aware semantic segmentation accuracy on PASCAL VOC. Meanwhile, our method takes only 360ms testing an image using VGG-16, which is two orders of magnitude faster than previous systems for this challenging problem. As a by product, our method also achieves compelling object detection results which surpass the competitive Fast/Faster R-CNN systems. The method described in this paper is the foundation of our submissions to the MS COCO 2015 segmentation competition, where we won the 1st place.},
  archiveprefix = {arXiv},
  eprint = {1512.04412},
  eprinttype = {arxiv},
  file = {/Users/anselm/Zotero/storage/4J7VPJL8/Dai et al. - 2015 - Instance-aware Semantic Segmentation via Multi-tas.pdf;/Users/anselm/Zotero/storage/QEZIKEX4/1512.html},
  journal = {arXiv:1512.04412 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  primaryclass = {cs}
}

@inproceedings{dalalHistogramsOrientedGradients2005,
  title = {Histograms of {{Oriented Gradients}} for {{Human Detection}}},
  booktitle = {2005 {{IEEE Computer Society Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}}'05)},
  author = {Dalal, N. and Triggs, B.},
  year = {2005},
  volume = {1},
  pages = {886--893},
  publisher = {{IEEE}},
  address = {{San Diego, CA, USA}},
  doi = {10.1109/CVPR.2005.177},
  abstract = {We study the question of feature sets for robust visual object recognition, adopting linear SVM based human detection as a test case. After reviewing existing edge and gradient based descriptors, we show experimentally that grids of Histograms of Oriented Gradient (HOG) descriptors significantly outperform existing feature sets for human detection. We study the influence of each stage of the computation on performance, concluding that fine-scale gradients, fine orientation binning, relatively coarse spatial binning, and high-quality local contrast normalization in overlapping descriptor blocks are all important for good results. The new approach gives near-perfect separation on the original MIT pedestrian database, so we introduce a more challenging dataset containing over 1800 annotated human images with a large range of pose variations and backgrounds.},
  file = {/Users/anselm/Zotero/storage/EFTFBBQ2/Dalal and Triggs - 2005 - Histograms of Oriented Gradients for Human Detecti.pdf},
  isbn = {978-0-7695-2372-9},
  language = {en}
}

@incollection{daveTAOLargeScaleBenchmark2020,
  title = {{{TAO}}: {{A Large}}-{{Scale Benchmark}} for {{Tracking Any Object}}},
  shorttitle = {{{TAO}}},
  booktitle = {Computer {{Vision}} \textendash{} {{ECCV}} 2020},
  author = {Dave, Achal and Khurana, Tarasha and Tokmakov, Pavel and Schmid, Cordelia and Ramanan, Deva},
  editor = {Vedaldi, Andrea and Bischof, Horst and Brox, Thomas and Frahm, Jan-Michael},
  year = {2020},
  volume = {12350},
  pages = {436--454},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-030-58558-7_26},
  abstract = {For many years, multi-object tracking benchmarks have focused on a handful of categories. Motivated primarily by surveillance and self-driving applications, these datasets provide tracks for people, vehicles, and animals, ignoring the vast majority of objects in the world. By contrast, in the related field of object detection, the introduction of large-scale, diverse datasets (e.g., COCO) have fostered significant progress in developing highly robust solutions. To bridge this gap, we introduce a similarly diverse dataset for Tracking Any Object (TAO)4. It consists of 2,907 high resolution videos, captured in diverse environments, which are half a minute long on average. Importantly, we adopt a bottomup approach for discovering a large vocabulary of 833 categories, an order of magnitude more than prior tracking benchmarks. To this end, we ask annotators to label objects that move at any point in the video, and give names to them post factum. Our vocabulary is both significantly larger and qualitatively different from existing tracking datasets. To ensure scalability of annotation, we employ a federated approach that focuses manual effort on labeling tracks for those relevant objects in a video (e.g., those that move). We perform an extensive evaluation of state-ofthe-art trackers and make a number of important discoveries regarding large-vocabulary tracking in an open-world. In particular, we show that existing single- and multi-object trackers struggle when applied to this scenario in the wild, and that detection-based, multi-object trackers are in fact competitive with user-initialized ones. We hope that our dataset and analysis will boost further progress in the tracking community.},
  file = {/Users/anselm/Zotero/storage/JW6D8XZ3/Dave et al. - 2020 - TAO A Large-Scale Benchmark for Tracking Any Obje.pdf},
  isbn = {978-3-030-58557-0 978-3-030-58558-7},
  language = {en}
}

@article{dendorferMOTChallengeBenchmarkSingleCamera2020,
  title = {{{MOTChallenge}}: {{A Benchmark}} for {{Single}}-{{Camera Multiple Target Tracking}}},
  shorttitle = {{{MOTChallenge}}},
  author = {Dendorfer, Patrick and O{\v s}ep, Aljo{\v s}a and Milan, Anton and Schindler, Konrad and Cremers, Daniel and Reid, Ian and Roth, Stefan and {Leal-Taix{\'e}}, Laura},
  year = {2020},
  month = dec,
  abstract = {Standardized benchmarks have been crucial in pushing the performance of computer vision algorithms, especially since the advent of deep learning. Although leaderboards should not be over-claimed, they often provide the most objective measure of performance and are therefore important guides for research. We present MOTChallenge, a benchmark for single-camera Multiple Object Tracking (MOT) launched in late 2014, to collect existing and new data, and create a framework for the standardized evaluation of multiple object tracking methods. The benchmark is focused on multiple people tracking, since pedestrians are by far the most studied object in the tracking community, with applications ranging from robot navigation to self-driving cars. This paper collects the first three releases of the benchmark: (i) MOT15, along with numerous state-of-the-art results that were submitted in the last years, (ii) MOT16, which contains new challenging videos, and (iii) MOT17, that extends MOT16 sequences with more precise labels and evaluates tracking performance on three different object detectors. The second and third release not only offers a significant increase in the number of labeled boxes but also provide labels for multiple object classes beside pedestrians, as well as the level of visibility for every single object of interest. We finally provide a categorization of state-of-the-art trackers and a broad error analysis. This will help newcomers understand the related work and research trends in the MOT community, and hopefully shed some light on potential future research directions.},
  archiveprefix = {arXiv},
  eprint = {2010.07548},
  eprinttype = {arxiv},
  file = {/Users/anselm/Zotero/storage/S8JA38LJ/Dendorfer et al. - 2020 - MOTChallenge A Benchmark for Single-Camera Multip.pdf},
  journal = {arXiv:2010.07548 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  language = {en},
  primaryclass = {cs}
}

@article{dericheUsingCannyCriteria1987,
  title = {Using {{Canny}}'s Criteria to Derive a Recursively Implemented Optimal Edge Detector},
  author = {Deriche, Rachid},
  year = {1987},
  volume = {1},
  pages = {167--187},
  issn = {0920-5691, 1573-1405},
  doi = {10.1007/BF00123164},
  abstract = {A highly efficient recursive algorithm for edge detection is presented. Using Canny's design [1], we show that a solution to his precise formulation of detection and localization for an infinite extent filter leads to an optimal operator in one dimension, which can be efficiently implemented by two recursive filters moving in opposite directions. In addition to the noise truncature immunity which results, the recursive nature of the filtering operations leads, with sequential machines, to a substantial saving in computational effort (five multiplications and five additions for one pixel, independent of the size of the neighborhood). The extension to the two-dimensional case is considered and the resulting filtering structures are implemented as two-dimensional recursive filters. Hence, the filter size can be varied by simply changing the value of one parameter without affecting the time execution of the algorithm. Performance measures of this new edge detector are given and compared to Canny's filters. Various experimental results are shown.},
  file = {/Users/anselm/Zotero/storage/JAHESJJW/Deriche - 1987 - Using Canny's criteria to derive a recursively imp.pdf},
  journal = {International Journal of Computer Vision},
  language = {en},
  number = {2}
}

@article{detoneSuperPointSelfSupervisedInterest2018,
  title = {{{SuperPoint}}: {{Self}}-{{Supervised Interest Point Detection}} and {{Description}}},
  shorttitle = {{{SuperPoint}}},
  author = {DeTone, Daniel and Malisiewicz, Tomasz and Rabinovich, Andrew},
  year = {2018},
  month = apr,
  abstract = {This paper presents a self-supervised framework for training interest point detectors and descriptors suitable for a large number of multiple-view geometry problems in computer vision. As opposed to patch-based neural networks, our fully-convolutional model operates on full-sized images and jointly computes pixel-level interest point locations and associated descriptors in one forward pass. We introduce Homographic Adaptation, a multi-scale, multi-homography approach for boosting interest point detection repeatability and performing cross-domain adaptation (e.g., synthetic-to-real). Our model, when trained on the MS-COCO generic image dataset using Homographic Adaptation, is able to repeatedly detect a much richer set of interest points than the initial pre-adapted deep model and any other traditional corner detector. The final system gives rise to state-of-the-art homography estimation results on HPatches when compared to LIFT, SIFT and ORB.},
  archiveprefix = {arXiv},
  eprint = {1712.07629},
  eprinttype = {arxiv},
  file = {/Users/anselm/Zotero/storage/D35CFR2E/DeTone et al. - 2018 - SuperPoint Self-Supervised Interest Point Detecti.pdf;/Users/anselm/Zotero/storage/HCSYU4FR/1712.html},
  journal = {arXiv:1712.07629 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  primaryclass = {cs}
}

@article{dimAlternativeApproachSatellite2013,
  title = {Alternative {{Approach}} for {{Satellite Cloud Classification}}: {{Edge Gradient Application}}},
  shorttitle = {Alternative {{Approach}} for {{Satellite Cloud Classification}}},
  author = {Dim, Jules R. and Takamura, Tamio},
  year = {2013},
  volume = {2013},
  pages = {1--8},
  issn = {1687-9309, 1687-9317},
  doi = {10.1155/2013/584816},
  file = {/Users/anselm/Zotero/storage/8I4GXWSC/Dim and Takamura - 2013 - Alternative Approach for Satellite Cloud Classific.pdf},
  journal = {Advances in Meteorology},
  language = {en}
}

@article{dongVisualInertialSemanticSceneRepresentation2017,
  title = {Visual-{{Inertial}}-{{Semantic Scene Representation}} for 3-{{D Object Detection}}},
  author = {Dong, Jingming and Fei, Xiaohan and Soatto, Stefano},
  year = {2017},
  month = apr,
  abstract = {We describe a system to detect objects in three-dimensional space using video and inertial sensors (accelerometer and gyrometer), ubiquitous in modern mobile platforms from phones to drones. Inertials afford the ability to impose class-specific scale priors for objects, and provide a global orientation reference. A minimal sufficient representation, the posterior of semantic (identity) and syntactic (pose) attributes of objects in space, can be decomposed into a geometric term, which can be maintained by a localization-and-mapping filter, and a likelihood function, which can be approximated by a discriminatively-trained convolutional neural network. The resulting system can process the video stream causally in real time, and provides a representation of objects in the scene that is persistent: Confidence in the presence of objects grows with evidence, and objects previously seen are kept in memory even when temporarily occluded, with their return into view automatically predicted to prime re-detection.},
  archiveprefix = {arXiv},
  eprint = {1606.03968},
  eprinttype = {arxiv},
  file = {/Users/anselm/Zotero/storage/6L94RXFV/Dong et al. - 2017 - Visual-Inertial-Semantic Scene Representation for .pdf;/Users/anselm/Zotero/storage/A9D32YEU/1606.html},
  journal = {arXiv:1606.03968 [cs]},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition},
  primaryclass = {cs}
}

@article{dosovitskiyCARLAOpenUrban2017,
  title = {{{CARLA}}: {{An Open Urban Driving Simulator}}},
  shorttitle = {{{CARLA}}},
  author = {Dosovitskiy, Alexey and Ros, German and Codevilla, Felipe and Lopez, Antonio and Koltun, Vladlen},
  year = {2017},
  month = nov,
  abstract = {We introduce CARLA, an open-source simulator for autonomous driving research. CARLA has been developed from the ground up to support development, training, and validation of autonomous urban driving systems. In addition to open-source code and protocols, CARLA provides open digital assets (urban layouts, buildings, vehicles) that were created for this purpose and can be used freely. The simulation platform supports flexible specification of sensor suites and environmental conditions. We use CARLA to study the performance of three approaches to autonomous driving: a classic modular pipeline, an end-to-end model trained via imitation learning, and an end-to-end model trained via reinforcement learning. The approaches are evaluated in controlled scenarios of increasing difficulty, and their performance is examined via metrics provided by CARLA, illustrating the platform's utility for autonomous driving research. The supplementary video can be viewed at https://youtu.be/Hp8Dz-Zek2E},
  archiveprefix = {arXiv},
  eprint = {1711.03938},
  eprinttype = {arxiv},
  file = {/Users/anselm/Zotero/storage/XQ2XBAT8/Dosovitskiy et al. - 2017 - CARLA An Open Urban Driving Simulator.pdf;/Users/anselm/Zotero/storage/SMSIA6TF/1711.html},
  journal = {arXiv:1711.03938 [cs]},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Robotics},
  primaryclass = {cs}
}

@article{durrant-whyteSimultaneousLocalizationMapping2006,
  title = {Simultaneous Localization and Mapping: Part {{I}}},
  shorttitle = {Simultaneous Localization and Mapping},
  author = {{Durrant-Whyte}, H. and Bailey, T.},
  year = {2006},
  month = jun,
  volume = {13},
  pages = {99--110},
  issn = {1070-9932},
  doi = {10.1109/MRA.2006.1638022},
  file = {/Users/anselm/Zotero/storage/PKEG9U66/Durrant-Whyte and Bailey - 2006 - Simultaneous localization and mapping part I.pdf},
  journal = {IEEE Robotics \& Automation Magazine},
  language = {en},
  number = {2}
}

@article{eadeLieGroups2D,
  title = {Lie {{Groups}} for {{2D}} and {{3D Transformations}}},
  author = {Eade, Ethan},
  pages = {25},
  file = {/Users/anselm/Zotero/storage/S2C63PJS/Eade - Lie Groups for 2D and 3D Transformations.pdf},
  language = {en}
}

@article{engelDirectSparseOdometry2018,
  title = {Direct {{Sparse Odometry}}},
  author = {Engel, Jakob and Koltun, Vladlen and Cremers, Daniel},
  year = {2018},
  month = mar,
  volume = {40},
  pages = {611--625},
  issn = {0162-8828, 2160-9292},
  doi = {10.1109/TPAMI.2017.2658577},
  abstract = {Direct Sparse Odometry (DSO) is a visual odometry method based on a novel, highly accurate sparse and direct structure and motion formulation. It combines a fully direct probabilistic model (minimizing a photometric error) with consistent, joint optimization of all model parameters, including geometry-represented as inverse depth in a reference frame-and camera motion. This is achieved in real time by omitting the smoothness prior used in other direct methods and instead sampling pixels evenly throughout the images. Since our method does not depend on keypoint detectors or descriptors, it can naturally sample pixels from across all image regions that have intensity gradient, including edges or smooth intensity variations on essentially featureless walls. The proposed model integrates a full photometric calibration, accounting for exposure time, lens vignetting, and non-linear response functions. We thoroughly evaluate our method on three different datasets comprising several hours of video. The experiments show that the presented approach significantly outperforms state-of-the-art direct and indirect methods in a variety of real-world settings, both in terms of tracking accuracy and robustness.},
  file = {/Users/anselm/Zotero/storage/EN9VQ8QU/Engel et al. - 2018 - Direct Sparse Odometry.pdf},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  language = {en},
  number = {3}
}

@incollection{engelLSDSLAMLargeScaleDirect2014,
  title = {{{LSD}}-{{SLAM}}: {{Large}}-{{Scale Direct Monocular SLAM}}},
  shorttitle = {{{LSD}}-{{SLAM}}},
  booktitle = {Computer {{Vision}} \textendash{} {{ECCV}} 2014},
  author = {Engel, Jakob and Sch{\"o}ps, Thomas and Cremers, Daniel},
  editor = {Fleet, David and Pajdla, Tomas and Schiele, Bernt and Tuytelaars, Tinne},
  year = {2014},
  volume = {8690},
  pages = {834--849},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-319-10605-2_54},
  abstract = {We propose a direct (feature-less) monocular SLAM algorithm which, in contrast to current state-of-the-art regarding direct methods, allows to build large-scale, consistent maps of the environment. Along with highly accurate pose estimation based on direct image alignment, the 3D environment is reconstructed in real-time as pose-graph of keyframes with associated semi-dense depth maps. These are obtained by filtering over a large number of pixelwise small-baseline stereo comparisons. The explicitly scale-drift aware formulation allows the approach to operate on challenging sequences including large variations in scene scale. Major enablers are two key novelties: (1) a novel direct tracking method which operates on sim(3), thereby explicitly detecting scale-drift, and (2) an elegant probabilistic solution to include the effect of noisy depth values into tracking. The resulting direct monocular SLAM system runs in real-time on a CPU.},
  file = {/Users/anselm/Zotero/storage/JPF7WLFE/Engel et al. - 2014 - LSD-SLAM Large-Scale Direct Monocular SLAM.pdf},
  isbn = {978-3-319-10604-5 978-3-319-10605-2},
  language = {en}
}

@book{erdmannIntroductionLieAlgebras2006,
  title = {Introduction to {{Lie}} Algebras},
  author = {Erdmann, Karin and Wildon, Mark J.},
  year = {2006},
  publisher = {{Springer}},
  address = {{London}},
  isbn = {978-1-84628-040-5},
  keywords = {Lie algebras},
  lccn = {QA252.3 .E73 2006},
  series = {Springer Undergraduate Mathematics Series}
}

@article{fischlerRandomSampleConsensus1981,
  title = {Random Sample Consensus: A Paradigm for Model Fitting with Applications to Image Analysis and Automated Cartography},
  shorttitle = {Random Sample Consensus},
  author = {Fischler, Martin A. and Bolles, Robert C.},
  year = {1981},
  month = jun,
  volume = {24},
  pages = {381--395},
  issn = {0001-0782, 1557-7317},
  doi = {10.1145/358669.358692},
  journal = {Communications of the ACM},
  language = {en},
  number = {6}
}

@article{frostRecoveringStableScale2018,
  title = {Recovering {{Stable Scale}} in {{Monocular SLAM Using Object}}-{{Supplemented Bundle Adjustment}}},
  author = {Frost, Duncan and Prisacariu, Victor and Murray, David},
  year = {2018},
  month = jun,
  volume = {34},
  pages = {736--747},
  issn = {1552-3098, 1941-0468},
  doi = {10.1109/TRO.2018.2820722},
  file = {/Users/anselm/Zotero/storage/MBPVVK8E/Frost et al. - 2018 - Recovering Stable Scale in Monocular SLAM Using Ob.pdf},
  journal = {IEEE Transactions on Robotics},
  number = {3}
}

@article{geiger3DTrafficScene2014,
  title = {{{3D Traffic Scene Understanding From Movable Platforms}}},
  author = {Geiger, Andreas and Lauer, Martin and Wojek, Christian and Stiller, Christoph and Urtasun, Raquel},
  year = {2014},
  month = may,
  volume = {36},
  pages = {1012--1025},
  issn = {0162-8828, 2160-9292},
  doi = {10.1109/TPAMI.2013.185},
  file = {/Users/anselm/Zotero/storage/9HJAUE3D/Geiger et al. - 2014 - 3D Traffic Scene Understanding From Movable Platfo.pdf},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number = {5}
}

@article{geigerVisionMeetsRobotics2013,
  title = {Vision Meets Robotics: {{The KITTI}} Dataset},
  shorttitle = {Vision Meets Robotics},
  author = {Geiger, A and Lenz, P and Stiller, C and Urtasun, R},
  year = {2013},
  month = sep,
  volume = {32},
  pages = {1231--1237},
  issn = {0278-3649, 1741-3176},
  doi = {10.1177/0278364913491297},
  abstract = {We present a novel dataset captured from a VW station wagon for use in mobile robotics and autonomous driving research. In total, we recorded 6 hours of traffic scenarios at 10-100 Hz using a variety of sensor modalities such as highresolution color and grayscale stereo cameras, a Velodyne 3D laser scanner and a high-precision GPS/IMU inertial navigation system. The scenarios are diverse, capturing real-world traffic situations and range from freeways over rural areas to innercity scenes with many static and dynamic objects. Our data is calibrated, synchronized and timestamped, and we provide the rectified and raw image sequences. Our dataset also contains object labels in the form of 3D tracklets and we provide online benchmarks for stereo, optical flow, object detection and other tasks. This paper describes our recording platform, the data format and the utilities that we provide.},
  file = {/Users/anselm/Zotero/storage/SNGTV2Q9/Geiger et al. - 2013 - Vision meets robotics The KITTI dataset.pdf},
  journal = {The International Journal of Robotics Research},
  language = {en},
  number = {11}
}

@book{giraltRoboticsResearchSeventh2000,
  title = {Robotics {{Research}}: The {{Seventh International Symposium}}},
  shorttitle = {Robotics {{Research}}},
  author = {Giralt, Georges and Hirzinger, Gerhard},
  year = {2000},
  publisher = {{Springer London}},
  address = {{London}},
  abstract = {This book is the proceedings of the 9th International Symposium of Robotics Research, one of the oldest and most prestigious conferences in robotics. The goal of the symposium was to bring together active, leading robotics researchers from academia, government and industry, to define the state of the art of robotics and its future direction. The broad spectrum of robotics research is covered, with an eye on what will be important in robotics in the next millennium.},
  annotation = {OCLC: 853258852},
  isbn = {978-1-4471-0765-1},
  language = {English}
}

@inproceedings{girshickRichFeatureHierarchies2014,
  title = {Rich {{Feature Hierarchies}} for {{Accurate Object Detection}} and {{Semantic Segmentation}}},
  booktitle = {2014 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Girshick, Ross and Donahue, Jeff and Darrell, Trevor and Malik, Jitendra},
  year = {2014},
  month = jun,
  pages = {580--587},
  publisher = {{IEEE}},
  address = {{Columbus, OH, USA}},
  doi = {10.1109/CVPR.2014.81},
  abstract = {Object detection performance, as measured on the canonical PASCAL VOC dataset, has plateaued in the last few years. The best-performing methods are complex ensemble systems that typically combine multiple low-level image features with high-level context. In this paper, we propose a simple and scalable detection algorithm that improves mean average precision (mAP) by more than 30\% relative to the previous best result on VOC 2012\textemdash achieving a mAP of 53.3\%. Our approach combines two key insights: (1) one can apply high-capacity convolutional neural networks (CNNs) to bottom-up region proposals in order to localize and segment objects and (2) when labeled training data is scarce, supervised pre-training for an auxiliary task, followed by domain-specific fine-tuning, yields a significant performance boost. Since we combine region proposals with CNNs, we call our method R-CNN: Regions with CNN features. We also present experiments that provide insight into what the network learns, revealing a rich hierarchy of image features. Source code for the complete system is available at http://www.cs.berkeley.edu/ \texttildelow rbg/rcnn.},
  file = {/Users/anselm/Zotero/storage/HK4W38YZ/Girshick et al. - 2014 - Rich Feature Hierarchies for Accurate Object Detec.pdf},
  isbn = {978-1-4799-5118-5},
  language = {en}
}

@book{GuideConvolutionalNeural2017,
  title = {Guide to Convolutional Neural Networks},
  year = {2017},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{New York, NY}},
  isbn = {978-3-319-57549-0}
}

@article{guivantLocalizationMapBuilding2000,
  title = {Localization and Map Building Using Laser Range Sensors in Outdoor Applications},
  author = {Guivant, Jose and Nebot, Eduardo and Baiker, Stephan},
  year = {2000},
  volume = {17},
  pages = {565--583},
  doi = {10.1002/1097-4563(200010)17:10<565::AID-ROB4>3.0.CO;2-6},
  abstract = {Abstract This paper presents the design of a high accuracy outdoor navigation system based on standard dead reckoning sensors and laser range and bearing information. The data validation problem is addressed using laser intensity information. The beacon design aspect and location of landmarks are also discussed in relation to desired accuracy and required area of operation. The results are important for simultaneous localization and map building applications (SLAM), since the feature extraction and validation are resolved at the sensor level using laser intensity. This facilitates the use of additional natural landmarks to improve the accuracy of the localization algorithm. The modelling aspects to implement SLAM with beacons and natural features are also presented. These results are of fundamental importance because the implementation of the algorithm does not require the surveying of beacons. Furthermore we demonstrate that by using natural landmarks accurate localization can be achieved by only requiring the initial estimate of the position of the vehicle. The algorithms are validated in outdoor environments using a standard utility car retrofitted with the navigation sensors and a 1 cm precision Kinematic GPS used as ground truth. \textcopyright{} 2000 John Wiley \& Sons, Inc.},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/1097-4563\%28200010\%2917\%3A10\%3C565\%3A\%3AAID-ROB4\%3E3.0.CO\%3B2-6},
  journal = {Journal of Robotic Systems},
  number = {10}
}

@inproceedings{gutmannIncrementalMappingLarge1999,
  title = {Incremental Mapping of Large Cyclic Environments},
  booktitle = {Proceedings 1999 {{IEEE International Symposium}} on {{Computational Intelligence}} in {{Robotics}} and {{Automation}}. {{CIRA}}'99 ({{Cat}}. {{No}}.{{99EX375}})},
  author = {Gutmann, J.-S. and Konolige, K.},
  year = {1999},
  pages = {318--325},
  publisher = {{IEEE}},
  address = {{Monterey, CA, USA}},
  doi = {10.1109/CIRA.1999.810068},
  file = {/Users/anselm/Zotero/storage/WCLTWEJR/Gutmann and Konolige - 1999 - Incremental mapping of large cyclic environments.pdf},
  isbn = {978-0-7803-5806-5}
}

@book{hallLieGroupsLie2015,
  title = {Lie Groups, {{Lie}} Algebras, and Representations: An Elementary Introduction},
  shorttitle = {Lie Groups, {{Lie}} Algebras, and Representations},
  author = {Hall, Brian C.},
  year = {2015},
  edition = {Second edition},
  publisher = {{Springer}},
  address = {{Cham ; New York}},
  annotation = {OCLC: ocn910324548},
  isbn = {978-3-319-13466-6},
  keywords = {Lie algebras,Lie groups,Representations of Lie algebras,Representations of Lie groups},
  lccn = {QA387 .H34 2015},
  number = {222},
  series = {Graduate Texts in Mathematics}
}

@inproceedings{harrisCombinedCornerEdge1988,
  title = {A {{Combined Corner}} and {{Edge Detector}}},
  booktitle = {Procedings of the {{Alvey Vision Conference}} 1988},
  author = {Harris, C. and Stephens, M.},
  year = {1988},
  pages = {23.1-23.6},
  publisher = {{Alvey Vision Club}},
  address = {{Manchester}},
  doi = {10.5244/C.2.23},
  file = {/Users/anselm/Zotero/storage/H66NYVRP/Harris and Stephens - 1988 - A Combined Corner and Edge Detector.pdf},
  language = {en}
}

@article{hartleyDefenseEightpointAlgorithm1997,
  title = {In Defense of the Eight-Point Algorithm},
  author = {Hartley, R.I.},
  year = {1997},
  month = jun,
  volume = {19},
  pages = {580--593},
  issn = {01628828},
  doi = {10.1109/34.601246},
  file = {/Users/anselm/Zotero/storage/5HZUBXY9/Hartley - 1997 - In defense of the eight-point algorithm.pdf},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number = {6}
}

@book{hartleyMultipleViewGeometry2004,
  title = {Multiple {{View Geometry}} in {{Computer Vision}}},
  author = {Hartley, Richard and Zisserman, Andrew},
  year = {2004},
  month = mar,
  edition = {Second},
  publisher = {{Cambridge University Press}},
  doi = {10.1017/CBO9780511811685},
  isbn = {978-0-521-54051-3 978-0-511-81168-5}
}

@article{hataCS231ACourseNotes,
  title = {{{CS231A Course Notes}} 3: {{Epipolar Geometry}}},
  author = {Hata, Kenji and Savarese, Silvio},
  pages = {14},
  file = {/Users/anselm/Zotero/storage/T6AKZVSK/Hata and Savarese - CS231A Course Notes 3 Epipolar Geometry.pdf},
  language = {en}
}

@article{heatonIanGoodfellowYoshua2018,
  title = {Ian {{Goodfellow}}, {{Yoshua Bengio}}, and {{Aaron Courville}}: {{Deep}} Learning: {{The MIT Press}}, 2016, 800 Pp, {{ISBN}}: 0262035618},
  shorttitle = {Ian {{Goodfellow}}, {{Yoshua Bengio}}, and {{Aaron Courville}}},
  author = {Heaton, Jeff},
  year = {2018},
  month = jun,
  volume = {19},
  pages = {305--307},
  issn = {1389-2576, 1573-7632},
  doi = {10.1007/s10710-017-9314-z},
  file = {/Users/anselm/Zotero/storage/UCBAHTY9/Heaton - 2018 - Ian Goodfellow, Yoshua Bengio, and Aaron Courville.pdf},
  journal = {Genetic Programming and Evolvable Machines},
  language = {en},
  number = {1-2}
}

@article{heMaskRCNN2018,
  title = {Mask {{R}}-{{CNN}}},
  author = {He, Kaiming and Gkioxari, Georgia and Doll{\'a}r, Piotr and Girshick, Ross},
  year = {2018},
  month = jan,
  abstract = {We present a conceptually simple, flexible, and general framework for object instance segmentation. Our approach efficiently detects objects in an image while simultaneously generating a high-quality segmentation mask for each instance. The method, called Mask R-CNN, extends Faster R-CNN by adding a branch for predicting an object mask in parallel with the existing branch for bounding box recognition. Mask R-CNN is simple to train and adds only a small overhead to Faster R-CNN, running at 5 fps. Moreover, Mask R-CNN is easy to generalize to other tasks, e.g., allowing us to estimate human poses in the same framework. We show top results in all three tracks of the COCO suite of challenges, including instance segmentation, bounding-box object detection, and person keypoint detection. Without bells and whistles, Mask R-CNN outperforms all existing, single-model entries on every task, including the COCO 2016 challenge winners. We hope our simple and effective approach will serve as a solid baseline and help ease future research in instance-level recognition. Code has been made available at: https://github.com/facebookresearch/Detectron},
  archiveprefix = {arXiv},
  eprint = {1703.06870},
  eprinttype = {arxiv},
  file = {/Users/anselm/Zotero/storage/KRK435N8/He et al. - 2018 - Mask R-CNN.pdf;/Users/anselm/Zotero/storage/2KIVRDEX/1703.html},
  journal = {arXiv:1703.06870 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  primaryclass = {cs}
}

@article{heneinDynamicSLAMNeed2020,
  title = {Dynamic {{SLAM}}: {{The Need For Speed}}},
  shorttitle = {Dynamic {{SLAM}}},
  author = {Henein, Mina and Zhang, Jun and Mahony, Robert and Ila, Viorela},
  year = {2020},
  month = feb,
  abstract = {The static world assumption is standard in most simultaneous localisation and mapping (SLAM) algorithms. Increased deployment of autonomous systems to unstructured dynamic environments is driving a need to identify moving objects and estimate their velocity in real-time. Most existing SLAM based approaches rely on a database of 3D models of objects or impose significant motion constraints. In this paper, we propose a new feature-based, model-free, object-aware dynamic SLAM algorithm that exploits semantic segmentation to allow estimation of motion of rigid objects in a scene without the need to estimate the object poses or have any prior knowledge of their 3D models. The algorithm generates a map of dynamic and static structure and has the ability to extract velocities of rigid moving objects in the scene. Its performance is demonstrated on simulated, synthetic and real-world datasets.},
  archiveprefix = {arXiv},
  eprint = {2002.08584},
  eprinttype = {arxiv},
  file = {/Users/anselm/Zotero/storage/S7T3U55Y/Henein et al. - 2020 - Dynamic SLAM The Need For Speed.pdf},
  journal = {arXiv:2002.08584 [cs]},
  keywords = {Computer Science - Robotics},
  language = {en},
  primaryclass = {cs}
}

@article{heneinExploitingRigidBody,
  title = {Exploiting {{Rigid Body Motion}} for {{SLAM}} in {{Dynamic Environments}}},
  author = {Henein, Mina and Kennedy, Gerard and Mahony, Robert and Ila, Viorela},
  pages = {8},
  abstract = {The limitations of existing localisation and mapping algorithms in handling highly dynamic environments is a key roadblock in the deployment of autonomous mobile robotic systems in a range of important real world situations. In this paper we propose a technique to integrate the motion of dynamic objects into a Simultaneous Localisation and Mapping (SLAM) algorithm without the need to know a-priori or model the geometry of the object, or even to explicitly estimate the pose of the object. The benefit of this approach lies in a simplification of the underlying SLAM state and a resulting simplification of the non-linear least squares optimisation solution. We demonstrate the performance of the algorithm on two scenarios; SLAM in an urban traffic scenario, and extrinsic calibration of a multi RGBD camera system observing a moving object. Our experiments show consistent improvement in robot localisation and mapping accuracy and demonstrate potential of the proposed algorithm.},
  file = {/Users/anselm/Zotero/storage/SNTFEFWQ/Henein et al. - Exploiting Rigid Body Motion for SLAM in Dynamic E.pdf},
  language = {en}
}

@article{henschelTrackingMultilevelFeatures2016,
  title = {Tracking with Multi-Level Features},
  author = {Henschel, Roberto and {Leal-Taix{\'e}}, Laura and Rosenhahn, Bodo and Schindler, Konrad},
  year = {2016},
  month = jul,
  abstract = {We present a novel formulation of the multiple object tracking problem which integrates low and mid-level features. In particular, we formulate the tracking problem as a quadratic program coupling detections and dense point trajectories. Due to the computational complexity of the initial QP, we propose an approximation by two auxiliary problems, a temporal and spatial association, where the temporal subproblem can be efficiently solved by a linear program and the spatial association by a clustering algorithm. The objective function of the QP is used in order to find the optimal number of clusters, where each cluster ideally represents one person. Evaluation is provided for multiple scenarios, showing the superiority of our method with respect to classic tracking-by-detection methods and also other methods that greedily integrate low-level features.},
  archiveprefix = {arXiv},
  eprint = {1607.07304},
  eprinttype = {arxiv},
  file = {/Users/anselm/Zotero/storage/YABCELJH/Henschel et al. - 2016 - Tracking with multi-level features.pdf;/Users/anselm/Zotero/storage/8WFHZ7N7/1607.html},
  journal = {arXiv:1607.07304 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  primaryclass = {cs}
}

@misc{hirzingerRoboticsResearchSeventh0000,
  title = {Robotics {{Research}}: {{The Seventh International Symposium}}},
  shorttitle = {Robotics {{Research}}},
  author = {Hirzinger, Gerhard and Giralt, Georges},
  year = {0000 uu},
  annotation = {OCLC: 1159737411},
  isbn = {9781447107651},
  language = {English}
}

@book{hollerbachRoboticsResearch2000,
  title = {Robotics {{Research}}},
  editor = {Hollerbach, John M. and Koditschek, Daniel E.},
  year = {2000},
  publisher = {{Springer London}},
  address = {{London}},
  doi = {10.1007/978-1-4471-0765-1},
  isbn = {978-1-4471-1254-9 978-1-4471-0765-1},
  language = {en}
}

@inproceedings{huangClusterSLAMSLAMBackend2019,
  title = {{{ClusterSLAM}}: {{A SLAM Backend}} for {{Simultaneous Rigid Body Clustering}} and {{Motion Estimation}}},
  shorttitle = {{{ClusterSLAM}}},
  booktitle = {2019 {{IEEE}}/{{CVF International Conference}} on {{Computer Vision}} ({{ICCV}})},
  author = {Huang, Jiahui and Yang, Sheng and Zhao, Zishuo and Lai, Yu-Kun and Hu, Shimin},
  year = {2019},
  month = oct,
  pages = {5874--5883},
  publisher = {{IEEE}},
  address = {{Seoul, Korea (South)}},
  doi = {10.1109/ICCV.2019.00597},
  abstract = {We present a practical backend for stereo visual SLAM which can simultaneously discover individual rigid bodies and compute their motions in dynamic environments. While recent factor graph based state optimization algorithms have shown their ability to robustly solve SLAM problems by treating dynamic objects as outliers, the dynamic motions are rarely considered. In this paper, we exploit the consensus of 3D motions among the landmarks extracted from the same rigid body for clustering and estimating static and dynamic objects in a unified manner. Specifically, our algorithm builds a noise-aware motion affinity matrix upon landmarks, and uses agglomerative clustering for distinguishing those rigid bodies. Accompanied by a decoupled factor graph optimization for revising their shape and trajectory, we obtain an iterative scheme to update both cluster assignments and motion estimation reciprocally. Evaluations on both synthetic scenes and KITTI demonstrate the capability of our approach, and further experiments considering online efficiency also show the effectiveness of our method for simultaneous tracking of egomotion and multiple objects.},
  file = {/Users/anselm/Zotero/storage/5J5YRUTS/Huang et al. - 2019 - ClusterSLAM A SLAM Backend for Simultaneous Rigid.pdf},
  isbn = {978-1-72814-803-8},
  language = {en}
}

@article{huberRobustEstimationLocation1964,
  title = {Robust {{Estimation}} of a {{Location Parameter}}},
  author = {Huber, Peter J.},
  year = {1964},
  month = mar,
  volume = {35},
  pages = {73--101},
  issn = {0003-4851},
  doi = {10.1214/aoms/1177703732},
  file = {/Users/anselm/Zotero/storage/GVRYUEKA/Huber - 1964 - Robust Estimation of a Location Parameter.pdf},
  journal = {The Annals of Mathematical Statistics},
  language = {en},
  number = {1}
}

@incollection{hutchisonBRIEFBinaryRobust2010,
  title = {{{BRIEF}}: {{Binary Robust Independent Elementary Features}}},
  shorttitle = {{{BRIEF}}},
  booktitle = {Computer {{Vision}} \textendash{} {{ECCV}} 2010},
  author = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Doug and Vardi, Moshe Y. and Weikum, Gerhard and Calonder, Michael and Lepetit, Vincent and Strecha, Christoph and Fua, Pascal},
  editor = {Daniilidis, Kostas and Maragos, Petros and Paragios, Nikos},
  year = {2010},
  volume = {6314},
  pages = {778--792},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-642-15561-1_56},
  file = {/Users/anselm/Zotero/storage/RQHMWARW/Hutchison et al. - 2010 - BRIEF Binary Robust Independent Elementary Featur.pdf},
  isbn = {978-3-642-15560-4 978-3-642-15561-1},
  language = {en}
}

@book{ieeeindustrialelectronicssocietyProceedingsIROS911991,
  title = {Proceedings: {{IROS}} '91, {{IEEE}}/{{RSJ International Workshop}} on {{Intelligent Robots}} and {{Systems}} '91: Intelligence for Mechanical Systems: {{November}} 3-5, 1991, {{International House Osaka}}, {{Osaka}}, {{Japan}}},
  shorttitle = {Proceedings},
  editor = {IEEE Industrial Electronics Society},
  year = {1991},
  publisher = {{Institute of Electrical and Electronics Engineers}},
  address = {{New York}},
  isbn = {978-0-7803-0067-5 978-0-7803-0068-2},
  keywords = {Congresses,Intelligent control systems,Robotics},
  lccn = {TJ210.3 .I443 1991}
}

@book{ieeeroboticsandautomationsocietyProceedings1999IEEE1999,
  title = {Proceedings, 1999 {{IEEE International Symposium}} on {{Computational Intelligence}} in {{Robotics}} and {{Automation}}: {{CIRA}} '99: {{November}} 8-9, 1999, {{Monterey}}, {{California}}, {{USA}}},
  shorttitle = {Proceedings, 1999 {{IEEE International Symposium}} on {{Computational Intelligence}} in {{Robotics}} and {{Automation}}},
  editor = {{IEEE Robotics {and} Automation Society} and {Institute of Electrical {and} Electronics Engineers}},
  year = {1999},
  publisher = {{IEEE}},
  address = {{Piscataway, NJ}},
  isbn = {978-0-7803-5806-5},
  keywords = {Automatic control,Computational intelligence,Congresses,Robotics},
  lccn = {TJ210.3 .I442 1999}
}

@article{jaccardDISTRIBUTIONFLORAALPINE1912,
  title = {{{THE DISTRIBUTION OF THE FLORA IN THE ALPINE ZONE}}.1},
  author = {Jaccard, Paul},
  year = {1912},
  month = feb,
  volume = {11},
  pages = {37--50},
  issn = {0028-646X, 1469-8137},
  doi = {10.1111/j.1469-8137.1912.tb05611.x},
  journal = {New Phytologist},
  language = {en},
  number = {2}
}

@book{jahneComputerVisionApplications2000,
  title = {Computer Vision and Applications: A Guide for Students and Practitioners},
  shorttitle = {Computer Vision and Applications},
  editor = {J{\"a}hne, Bernd and Haussecker, Horst},
  year = {2000},
  publisher = {{Academic Press}},
  address = {{San Diego}},
  isbn = {978-0-12-379777-3},
  keywords = {Computer vision},
  lccn = {TA1634 .C648 2000}
}

@article{kahlerVeryHighFrame2015,
  title = {Very {{High Frame Rate Volumetric Integration}} of {{Depth Images}} on {{Mobile Devices}}},
  author = {Kahler, Olaf and Adrian Prisacariu, Victor and Yuheng Ren, Carl and Sun, Xin and Torr, Philip and Murray, David},
  year = {2015},
  month = nov,
  volume = {21},
  pages = {1241--1250},
  issn = {1077-2626, 1941-0506, 2160-9306},
  doi = {10.1109/TVCG.2015.2459891},
  journal = {IEEE Transactions on Visualization and Computer Graphics},
  number = {11}
}

@inproceedings{kerlRobustOdometryEstimation2013,
  title = {Robust Odometry Estimation for {{RGB}}-{{D}} Cameras},
  booktitle = {2013 {{IEEE International Conference}} on {{Robotics}} and {{Automation}}},
  author = {Kerl, Christian and Sturm, Jurgen and Cremers, Daniel},
  year = {2013},
  month = may,
  pages = {3748--3754},
  publisher = {{IEEE}},
  address = {{Karlsruhe, Germany}},
  doi = {10.1109/ICRA.2013.6631104},
  abstract = {The goal of our work is to provide a fast and accurate method to estimate the camera motion from RGB-D images. Our approach registers two consecutive RGB-D frames directly upon each other by minimizing the photometric error. We estimate the camera motion using non-linear minimization in combination with a coarse-to-fine scheme. To allow for noise and outliers in the image data, we propose to use a robust error function that reduces the influence of large residuals. Furthermore, our formulation allows for the inclusion of a motion model which can be based on prior knowledge, temporal filtering, or additional sensors like an IMU. Our method is attractive for robots with limited computational resources as it runs in real-time on a single CPU core and has a small, constant memory footprint. In an extensive set of experiments carried out both on a benchmark dataset and synthetic data, we demonstrate that our approach is more accurate and robust than previous methods. We provide our software under an open source license.},
  file = {/Users/anselm/Zotero/storage/N2U8Y3E8/Kerl et al. - 2013 - Robust odometry estimation for RGB-D cameras.pdf},
  isbn = {978-1-4673-5643-5 978-1-4673-5641-1},
  language = {en}
}

@article{kimEffectiveBackgroundModelBased2016,
  title = {Effective {{Background Model}}-{{Based RGB}}-{{D Dense Visual Odometry}} in a {{Dynamic Environment}}},
  author = {Kim, Deok-Hwa and Kim, Jong-Hwan},
  year = {2016},
  month = dec,
  volume = {32},
  pages = {1565--1573},
  issn = {1552-3098, 1941-0468},
  doi = {10.1109/TRO.2016.2609395},
  journal = {IEEE Transactions on Robotics},
  number = {6}
}

@article{krizhevskyImageNetClassificationDeep2017,
  title = {{{ImageNet}} Classification with Deep Convolutional Neural Networks},
  author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E.},
  year = {2017},
  month = may,
  volume = {60},
  pages = {84--90},
  issn = {0001-0782, 1557-7317},
  doi = {10.1145/3065386},
  file = {/Users/anselm/Zotero/storage/3JZIVNBJ/Krizhevsky et al. - 2017 - ImageNet classification with deep convolutional ne.pdf},
  journal = {Communications of the ACM},
  language = {en},
  number = {6}
}

@article{kuhnHungarianMethodAssignment1955,
  title = {The {{Hungarian}} Method for the Assignment Problem},
  author = {Kuhn, H. W.},
  year = {1955},
  month = mar,
  volume = {2},
  pages = {83--97},
  issn = {00281441, 19319193},
  doi = {10.1002/nav.3800020109},
  file = {/Users/anselm/Zotero/storage/3CPK9TXM/Kuhn - 1955 - The Hungarian method for the assignment problem.pdf},
  journal = {Naval Research Logistics Quarterly},
  language = {en},
  number = {1-2}
}

@article{kuJoint3DProposal2018,
  title = {Joint {{3D Proposal Generation}} and {{Object Detection}} from {{View Aggregation}}},
  author = {Ku, Jason and Mozifian, Melissa and Lee, Jungwook and Harakeh, Ali and Waslander, Steven},
  year = {2018},
  month = jul,
  abstract = {We present AVOD, an Aggregate View Object Detection network for autonomous driving scenarios. The proposed neural network architecture uses LIDAR point clouds and RGB images to generate features that are shared by two subnetworks: a region proposal network (RPN) and a second stage detector network. The proposed RPN uses a novel architecture capable of performing multimodal feature fusion on high resolution feature maps to generate reliable 3D object proposals for multiple object classes in road scenes. Using these proposals, the second stage detection network performs accurate oriented 3D bounding box regression and category classification to predict the extents, orientation, and classification of objects in 3D space. Our proposed architecture is shown to produce state of the art results on the KITTI 3D object detection benchmark while running in real time with a low memory footprint, making it a suitable candidate for deployment on autonomous vehicles. Code is at: https://github.com/kujason/avod},
  archiveprefix = {arXiv},
  eprint = {1712.02294},
  eprinttype = {arxiv},
  file = {/Users/anselm/Zotero/storage/W687CQ9F/Ku et al. - 2018 - Joint 3D Proposal Generation and Object Detection .pdf;/Users/anselm/Zotero/storage/N2LL7NGU/1712.html},
  journal = {arXiv:1712.02294 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  primaryclass = {cs}
}

@inproceedings{kummerleG2oGeneralFramework2011,
  title = {G{\textsuperscript{2}}o: {{A}} General Framework for Graph Optimization},
  shorttitle = {G{\textsuperscript{2}}o},
  booktitle = {2011 {{IEEE International Conference}} on {{Robotics}} and {{Automation}}},
  author = {Kummerle, Rainer and Grisetti, Giorgio and Strasdat, Hauke and Konolige, Kurt and Burgard, Wolfram},
  year = {2011},
  month = may,
  pages = {3607--3613},
  publisher = {{IEEE}},
  address = {{Shanghai, China}},
  doi = {10.1109/ICRA.2011.5979949},
  abstract = {Many popular problems in robotics and computer vision including various types of simultaneous localization and mapping (SLAM) or bundle adjustment (BA) can be phrased as least squares optimization of an error function that can be represented by a graph. This paper describes the general structure of such problems and presents g2o, an open-source C++ framework for optimizing graph-based nonlinear error functions. Our system has been designed to be easily extensible to a wide range of problems and a new problem typically can be specified in a few lines of code. The current implementation provides solutions to several variants of SLAM and BA. We provide evaluations on a wide range of real-world and simulated datasets. The results demonstrate that while being general g2o offers a performance comparable to implementations of stateof-the-art approaches for the specific problems.},
  file = {/Users/anselm/Zotero/storage/7Q8VFPBR/Kummerle et al. - 2011 - G2o A general framework for graph opti.pdf},
  isbn = {978-1-61284-386-5},
  language = {en}
}

@book{langAlgebra2002,
  title = {Algebra},
  author = {Lang, Serge},
  year = {2002},
  edition = {Rev. 3rd ed},
  publisher = {{Springer}},
  address = {{New York}},
  isbn = {978-0-387-95385-4},
  keywords = {Algebra},
  lccn = {QA154.3 .L3 2002},
  number = {211},
  series = {Graduate Texts in Mathematics}
}

@inproceedings{leal-taixeEverybodyNeedsSomebody2011,
  title = {Everybody Needs Somebody: {{Modeling}} Social and Grouping Behavior on a Linear Programming Multiple People Tracker},
  shorttitle = {Everybody Needs Somebody},
  booktitle = {2011 {{IEEE International Conference}} on {{Computer Vision Workshops}} ({{ICCV Workshops}})},
  author = {{Leal-Taixe}, Laura and {Pons-Moll}, Gerard and Rosenhahn, Bodo},
  year = {2011},
  month = nov,
  pages = {120--127},
  publisher = {{IEEE}},
  address = {{Barcelona, Spain}},
  doi = {10.1109/ICCVW.2011.6130233},
  abstract = {Multiple people tracking consists in detecting the subjects at each frame and matching these detections to obtain full trajectories. In semi-crowded environments, pedestrians often occlude each other, making tracking a challenging task. Most tracking methods make the assumption that each pedestrian's motion is independent, thereby ignoring the complex and important interaction between subjects.},
  file = {/Users/anselm/Zotero/storage/Q5Y6RFDY/Leal-Taixe et al. - 2011 - Everybody needs somebody Modeling social and grou.pdf},
  isbn = {978-1-4673-0063-6 978-1-4673-0062-9 978-1-4673-0061-2},
  language = {en}
}

@article{lecunGradientbasedLearningApplied1998,
  title = {Gradient-Based Learning Applied to Document Recognition},
  author = {Lecun, Y. and Bottou, L. and Bengio, Y. and Haffner, P.},
  year = {Nov./1998},
  volume = {86},
  pages = {2278--2324},
  issn = {00189219},
  doi = {10.1109/5.726791},
  file = {/Users/anselm/Zotero/storage/LKDUWWER/Lecun et al. - 1998 - Gradient-based learning applied to document recogn.pdf},
  journal = {Proceedings of the IEEE},
  number = {11}
}

@article{leichterMonotonicityErrorType2013,
  title = {Monotonicity and {{Error Type Differentiability}} in {{Performance Measures}} for {{Target Detection}} and {{Tracking}} in {{Video}}},
  author = {Leichter, I. and Krupka, E.},
  year = {2013},
  month = oct,
  volume = {35},
  pages = {2553--2560},
  issn = {0162-8828, 2160-9292},
  doi = {10.1109/TPAMI.2013.70},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number = {10}
}

@inproceedings{leonardSimultaneousMapBuilding1991,
  title = {Simultaneous Map Building and Localization for an Autonomous Mobile Robot},
  booktitle = {Proceedings {{IROS}} '91:{{IEEE}}/{{RSJ International Workshop}} on {{Intelligent Robots}} and {{Systems}} '91},
  author = {Leonard, J.J. and {Durrant-Whyte}, H.F.},
  year = {1991},
  pages = {1442--1447},
  publisher = {{IEEE}},
  address = {{Osaka, Japan}},
  doi = {10.1109/IROS.1991.174711},
  isbn = {978-0-7803-0067-5}
}

@article{lepetitEPnPAccurateSolution2009,
  title = {{{EPnP}}: {{An Accurate O}}(n) {{Solution}} to the {{PnP Problem}}},
  shorttitle = {{{EPnP}}},
  author = {Lepetit, Vincent and {Moreno-Noguer}, Francesc and Fua, Pascal},
  year = {2009},
  month = feb,
  volume = {81},
  pages = {155--166},
  issn = {0920-5691, 1573-1405},
  doi = {10.1007/s11263-008-0152-6},
  file = {/Users/anselm/Zotero/storage/62SLTK5G/Lepetit et al. - 2009 - EPnP An Accurate O(n) Solution to the PnP Problem.pdf},
  journal = {International Journal of Computer Vision},
  language = {en},
  number = {2}
}

@article{levenbergMethodSolutionCertain1944,
  title = {A Method for the Solution of Certain Non-Linear Problems in Least Squares},
  author = {Levenberg, Kenneth},
  year = {1944},
  month = jul,
  volume = {2},
  pages = {164--168},
  issn = {0033-569X, 1552-4485},
  doi = {10.1090/qam/10666},
  file = {/Users/anselm/Zotero/storage/NGNQT552/Levenberg - 1944 - A method for the solution of certain non-linear pr.pdf},
  journal = {Quarterly of Applied Mathematics},
  language = {en},
  number = {2}
}

@article{liRGBDSLAMDynamic2017,
  title = {{{RGB}}-{{D SLAM}} in {{Dynamic Environments Using Static Point Weighting}}},
  author = {Li, Shile and Lee, Dongheui},
  year = {2017},
  month = oct,
  volume = {2},
  pages = {2263--2270},
  issn = {2377-3766, 2377-3774},
  doi = {10.1109/LRA.2017.2724759},
  journal = {IEEE Robotics and Automation Letters},
  number = {4}
}

@article{liStereoVisionbasedSemantic,
  title = {Stereo {{Vision}}-Based {{Semantic 3D Object}} and {{Ego}}-Motion {{Tracking}} for {{Autonomous Driving}}},
  author = {Li, Peiliang and Qin, Tong and Shen, Shaojie},
  pages = {16},
  abstract = {We propose a stereo vision-based approach for tracking the camera ego-motion and 3D semantic objects in dynamic autonomous driving scenarios. Instead of directly regressing the 3D bounding box using end-to-end approaches, we propose to use the easy-to-labeled 2D detection and discrete viewpoint classification together with a light-weight semantic inference method to obtain rough 3D object measurements. Based on the object-aware-aided camera pose tracking which is robust in dynamic environments, in combination with our novel dynamic object bundle adjustment (BA) approach to fuse temporal sparse feature correspondences and the semantic 3D measurement model, we obtain 3D object pose, velocity and anchored dynamic point cloud estimation with instance accuracy and temporal consistency. The performance of our proposed method is demonstrated in diverse scenarios. Both the ego-motion estimation and object localization are compared with the state-of-of-theart solutions.},
  file = {/Users/anselm/Zotero/storage/UBY54UUR/Li et al. - Stereo Vision-based Semantic 3D Object and Ego-mot.pdf},
  language = {en}
}

@article{liStereoVisionbasedSemantic2018,
  title = {Stereo {{Vision}}-Based {{Semantic 3D Object}} and {{Ego}}-Motion {{Tracking}} for {{Autonomous Driving}}},
  author = {Li, Peiliang and Qin, Tong and Shen, Shaojie},
  year = {2018},
  month = nov,
  abstract = {We propose a stereo vision-based approach for tracking the camera ego-motion and 3D semantic objects in dynamic autonomous driving scenarios. Instead of directly regressing the 3D bounding box using end-to-end approaches, we propose to use the easy-to-labeled 2D detection and discrete viewpoint classification together with a light-weight semantic inference method to obtain rough 3D object measurements. Based on the object-aware-aided camera pose tracking which is robust in dynamic environments, in combination with our novel dynamic object bundle adjustment (BA) approach to fuse temporal sparse feature correspondences and the semantic 3D measurement model, we obtain 3D object pose, velocity and anchored dynamic point cloud estimation with instance accuracy and temporal consistency. The performance of our proposed method is demonstrated in diverse scenarios. Both the ego-motion estimation and object localization are compared with the state-of-of-the-art solutions.},
  archiveprefix = {arXiv},
  eprint = {1807.02062},
  eprinttype = {arxiv},
  file = {/Users/anselm/Zotero/storage/KGAKCXPE/Li et al. - 2018 - Stereo Vision-based Semantic 3D Object and Ego-mot.pdf;/Users/anselm/Zotero/storage/788BPJL2/1807.html},
  journal = {arXiv:1807.02062 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  primaryclass = {cs}
}

@inproceedings{lizhangGlobalDataAssociation2008,
  title = {Global Data Association for Multi-Object Tracking Using Network Flows},
  booktitle = {2008 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {{Li Zhang} and {Yuan Li} and Nevatia, Ramakant},
  year = {2008},
  month = jun,
  pages = {1--8},
  publisher = {{IEEE}},
  address = {{Anchorage, AK, USA}},
  doi = {10.1109/CVPR.2008.4587584},
  abstract = {We propose a network flow based optimization method for data association needed for multiple object tracking. The maximum-a-posteriori (MAP) data association problem is mapped into a cost-flow network with a non-overlap constraint on trajectories. The optimal data association is found by a min-cost flow algorithm in the network. The network is augmented to include an Explicit Occlusion Model(EOM) to track with long-term inter-object occlusions. A solution to the EOM-based network is found by an iterative approach built upon the original algorithm. Initialization and termination of trajectories and potential false observations are modeled by the formulation intrinsically. The method is efficient and does not require hypotheses pruning. Performance is compared with previous results on two public pedestrian datasets to show its improvement.},
  file = {/Users/anselm/Zotero/storage/RFCFU5YY/Li Zhang et al. - 2008 - Global data association for multi-object tracking .pdf},
  isbn = {978-1-4244-2242-5},
  language = {en}
}

@article{longquanLinearNpointCamera1999,
  title = {Linear {{N}}-Point Camera Pose Determination},
  author = {{Long Quan} and {Zhongdan Lan}},
  year = {Aug./1999},
  volume = {21},
  pages = {774--780},
  issn = {01628828},
  doi = {10.1109/34.784291},
  abstract = {\DH The determination of camera position and orientation from known correspondences of 3D reference points and their images is known as pose estimation in computer vision and space resection in photogrammetry. It is wellknown that from three corresponding points there are at most four algebraic solutions. Less appears to be known about the cases of four and five corresponding points. In this paper, we propose a family of linear methods that yield a unique solution to 4- and 5-point pose determination for generic reference points. We first review the 3-point algebraic method. Then we present our twostep, 4-point and one-step, 5-point linear algorithms. The 5-point method can also be extended to handle more than five points. Finally, we demonstrate our methods on both simulated and real images. We show that they do not degenerate for coplanar configurations and even outperform the special linear algorithm for coplanar configurations in practice.},
  file = {/Users/anselm/Zotero/storage/B4ZJ5ENS/Long Quan and Zhongdan Lan - 1999 - Linear N-point camera pose determination.pdf},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  language = {en},
  number = {8}
}

@article{longquanLinearNpointCamera1999a,
  title = {Linear {{N}}-Point Camera Pose Determination},
  author = {{Long Quan} and {Zhongdan Lan}},
  year = {Aug./1999},
  volume = {21},
  pages = {774--780},
  issn = {01628828},
  doi = {10.1109/34.784291},
  abstract = {\DH The determination of camera position and orientation from known correspondences of 3D reference points and their images is known as pose estimation in computer vision and space resection in photogrammetry. It is wellknown that from three corresponding points there are at most four algebraic solutions. Less appears to be known about the cases of four and five corresponding points. In this paper, we propose a family of linear methods that yield a unique solution to 4- and 5-point pose determination for generic reference points. We first review the 3-point algebraic method. Then we present our twostep, 4-point and one-step, 5-point linear algorithms. The 5-point method can also be extended to handle more than five points. Finally, we demonstrate our methods on both simulated and real images. We show that they do not degenerate for coplanar configurations and even outperform the special linear algorithm for coplanar configurations in practice.},
  file = {/Users/anselm/Zotero/storage/TUJWG5DD/Long Quan and Zhongdan Lan - 1999 - Linear N-point camera pose determination.pdf},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  language = {en},
  number = {8}
}

@article{longuet-higginsComputerAlgorithmReconstructing1981,
  title = {A Computer Algorithm for Reconstructing a Scene from Two Projections},
  author = {{Longuet-Higgins}, H. C.},
  year = {1981},
  month = sep,
  volume = {293},
  pages = {133--135},
  issn = {0028-0836, 1476-4687},
  doi = {10.1038/293133a0},
  journal = {Nature},
  language = {en},
  number = {5828}
}

@article{loweDistinctiveImageFeatures2004,
  title = {Distinctive {{Image Features}} from {{Scale}}-{{Invariant Keypoints}}},
  author = {Lowe, David G.},
  year = {2004},
  month = nov,
  volume = {60},
  pages = {91--110},
  issn = {0920-5691},
  doi = {10.1023/B:VISI.0000029664.99615.94},
  journal = {International Journal of Computer Vision},
  language = {en},
  number = {2}
}

@inproceedings{loweObjectRecognitionLocal1999,
  title = {Object Recognition from Local Scale-Invariant Features},
  booktitle = {Proceedings of the {{Seventh IEEE International Conference}} on {{Computer Vision}}},
  author = {Lowe, D.G.},
  year = {1999},
  pages = {1150-1157 vol.2},
  publisher = {{IEEE}},
  address = {{Kerkyra, Greece}},
  doi = {10.1109/ICCV.1999.790410},
  abstract = {An object recognition system has been developed that uses a new class of local image features. The features are invariant to image scaling, translation, and rotation, and partially invariant to illumination changes and affine or 3D projection. These features share similar properties with neurons in inferior temporal cortex that are used for object recognition in primate vision. Features are efficiently detected through a staged filtering approach that identifies stable points in scale space. Image keys are created that allow for local geometric deformations by representing blurred image gradients in multiple orientation planes and at multiple scales. The keys are used as input to a nearest-neighbor indexing method that identifies candidate object matches. Final verification of each match is achieved by finding a low-residual least-squares solution for the unknown model parameters. Experimental results show that robust object recognition can be achieved in cluttered partially-occluded images with a computation time of under 2 seconds.},
  file = {/Users/anselm/Zotero/storage/5I7TB8ZB/Lowe - 1999 - Object recognition from local scale-invariant feat.pdf},
  isbn = {978-0-7695-0164-2},
  language = {en}
}

@article{luitenHOTAHigherOrder2021,
  title = {{{HOTA}}: {{A Higher Order Metric}} for {{Evaluating Multi}}-Object {{Tracking}}},
  shorttitle = {{{HOTA}}},
  author = {Luiten, Jonathon and O{\u s}ep, Aljo{\u s}a and Dendorfer, Patrick and Torr, Philip and Geiger, Andreas and {Leal-Taix{\'e}}, Laura and Leibe, Bastian},
  year = {2021},
  month = feb,
  volume = {129},
  pages = {548--578},
  issn = {0920-5691, 1573-1405},
  doi = {10.1007/s11263-020-01375-2},
  abstract = {Multi-object tracking (MOT) has been notoriously difficult to evaluate. Previous metrics overemphasize the importance of either detection or association. To address this, we present a novel MOT evaluation metric, higher order tracking accuracy (HOTA), which explicitly balances the effect of performing accurate detection, association and localization into a single unified metric for comparing trackers. HOTA decomposes into a family of sub-metrics which are able to evaluate each of five basic error types separately, which enables clear analysis of tracking performance. We evaluate the effectiveness of HOTA on the MOTChallenge benchmark, and show that it is able to capture important aspects of MOT performance not previously taken into account by established metrics. Furthermore, we show HOTA scores better align with human visual evaluation of tracking performance.},
  file = {/Users/anselm/Zotero/storage/HWWBVPHU/Luiten et al. - 2021 - HOTA A Higher Order Metric for Evaluating Multi-o.pdf},
  journal = {International Journal of Computer Vision},
  language = {en},
  number = {2}
}

@article{luitenTrackReconstructReconstruct2020,
  title = {Track to {{Reconstruct}} and {{Reconstruct}} to {{Track}}},
  author = {Luiten, Jonathon and Fischer, Tobias and Leibe, Bastian},
  year = {2020},
  month = apr,
  doi = {10.1109/LRA.2020.2969183},
  abstract = {Object tracking and 3D reconstruction are often performed together, with tracking used as input for reconstruction. However, the obtained reconstructions also provide useful information for improving tracking. We propose a novel method that closes this loop, first tracking to reconstruct, and then reconstructing to track. Our approach, MOTSFusion (Multi-Object Tracking, Segmentation and dynamic object Fusion), exploits the 3D motion extracted from dynamic object reconstructions to track objects through long periods of complete occlusion and to recover missing detections. Our approach first builds up short tracklets using 2D optical flow, and then fuses these into dynamic 3D object reconstructions. The precise 3D object motion of these reconstructions is used to merge tracklets through occlusion into long-term tracks, and to locate objects when detections are missing. On KITTI, our reconstruction-based tracking reduces the number of ID switches of the initial tracklets by more than 50\%, and outperforms all previous approaches for both bounding box and segmentation tracking.},
  archiveprefix = {arXiv},
  eprint = {1910.00130},
  eprinttype = {arxiv},
  file = {/Users/anselm/Zotero/storage/EK6KDMSF/Luiten et al. - 2020 - Track to Reconstruct and Reconstruct to Track.pdf;/Users/anselm/Zotero/storage/M7NI55B5/1910.html},
  journal = {arXiv:1910.00130 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  primaryclass = {cs}
}

@article{marquardtAlgorithmLeastSquaresEstimation1963,
  title = {An {{Algorithm}} for {{Least}}-{{Squares Estimation}} of {{Nonlinear Parameters}}},
  author = {Marquardt, Donald W.},
  year = {1963},
  month = jun,
  volume = {11},
  pages = {431--441},
  issn = {0368-4245, 2168-3484},
  doi = {10.1137/0111030},
  journal = {Journal of the Society for Industrial and Applied Mathematics},
  language = {en},
  number = {2}
}

@book{mccarthyIntroductionTheoreticalKinematics1990,
  title = {An Introduction to Theoretical Kinematics},
  author = {McCarthy, J. M.},
  year = {1990},
  publisher = {{MIT Press}},
  address = {{Cambridge, Mass}},
  isbn = {978-0-262-13252-7},
  keywords = {Kinematics},
  lccn = {QA841 .M33 1990}
}

@patent{mcconnellMethodApparatusPattern,
  title = {Method of and Apparatus for Pattern Recognition},
  author = {McConnell, Robert K.},
  address = {{Arlington, Mass.}},
  assignee = {WaylandResearchInc.,Wayland, Mass.},
  file = {/Users/anselm/Zotero/storage/JYYPWXBW/US4567610.pdf},
  nationality = {US},
  number = {4,567,610}
}

@phdthesis{montemerloFastSLAMFactoredSolution,
  title = {{{FastSLAM}}: {{A Factored Solution}} to the {{Simultaneous Localization}} and {{Mapping Problem}}},
  author = {Montemerlo, Michael and Thrun, Sebastian and Koller, Daphne and Wegbreit, Ben},
  abstract = {The ability to simultaneously localize a robot and accurately map its surroundings is considered by many to be a key prerequisite of truly autonomous robots. However, few approaches to this problem scale up to handle the very large number of landmarks present in real environments. Kalman filter-based algorithms, for example, require time quadratic in the number of landmarks to incorporate each sensor observation. This paper presents FastSLAM, an algorithm that recursively estimates the full posterior distribution over robot pose and landmark locations, yet scales logarithmically with the number of landmarks in the map. This algorithm is based on an exact factorization of the posterior into a product of conditional landmark distributions and a distribution over robot paths. The algorithm has been run successfully on as many as 50,000 landmarks, environments far beyond the reach of previous approaches. Experimental results demonstrate the advantages and limitations of the FastSLAM algorithm on both simulated and realworld data.},
  file = {/Users/anselm/Zotero/storage/8XXNL9TE/Montemerlo et al. - FastSLAM A Factored Solution to the Simultaneous .pdf},
  language = {en},
  lccn = {CMU-RI-TR-03-28}
}

@article{moons3DReconstructionMultiple2008,
  title = {{{3D Reconstruction}} from {{Multiple Images Part}} 1: {{Principles}}},
  shorttitle = {{{3D Reconstruction}} from {{Multiple Images Part}} 1},
  author = {Moons, Theo},
  year = {2008},
  volume = {4},
  pages = {287--404},
  issn = {1572-2740, 1572-2759},
  doi = {10.1561/0600000007},
  file = {/Users/anselm/Zotero/storage/GTT46XXP/Moons - 2008 - 3D Reconstruction from Multiple Images Part 1 Pri.pdf},
  journal = {Foundations and Trends\textregistered{} in Computer Graphics and Vision},
  language = {en},
  number = {4}
}

@article{mousavian3DBoundingBox2017,
  title = {{{3D Bounding Box Estimation Using Deep Learning}} and {{Geometry}}},
  author = {Mousavian, Arsalan and Anguelov, Dragomir and Flynn, John and Kosecka, Jana},
  year = {2017},
  month = apr,
  abstract = {We present a method for 3D object detection and pose estimation from a single image. In contrast to current techniques that only regress the 3D orientation of an object, our method first regresses relatively stable 3D object properties using a deep convolutional neural network and then combines these estimates with geometric constraints provided by a 2D object bounding box to produce a complete 3D bounding box. The first network output estimates the 3D object orientation using a novel hybrid discrete-continuous loss, which significantly outperforms the L2 loss. The second output regresses the 3D object dimensions, which have relatively little variance compared to alternatives and can often be predicted for many object types. These estimates, combined with the geometric constraints on translation imposed by the 2D bounding box, enable us to recover a stable and accurate 3D object pose. We evaluate our method on the challenging KITTI object detection benchmark [2] both on the official metric of 3D orientation estimation and also on the accuracy of the obtained 3D bounding boxes. Although conceptually simple, our method outperforms more complex and computationally expensive approaches that leverage semantic segmentation, instance level segmentation and flat ground priors [4] and sub-category detection [23][24]. Our discrete-continuous loss also produces state of the art results for 3D viewpoint estimation on the Pascal 3D+ dataset[26].},
  archiveprefix = {arXiv},
  eprint = {1612.00496},
  eprinttype = {arxiv},
  file = {/Users/anselm/Zotero/storage/8SXKKC26/Mousavian et al. - 2017 - 3D Bounding Box Estimation Using Deep Learning and.pdf},
  journal = {arXiv:1612.00496 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  language = {en},
  primaryclass = {cs}
}

@article{mousavian3DBoundingBox2017a,
  title = {{{3D Bounding Box Estimation Using Deep Learning}} and {{Geometry}}},
  author = {Mousavian, Arsalan and Anguelov, Dragomir and Flynn, John and Kosecka, Jana},
  year = {2017},
  month = apr,
  abstract = {We present a method for 3D object detection and pose estimation from a single image. In contrast to current techniques that only regress the 3D orientation of an object, our method first regresses relatively stable 3D object properties using a deep convolutional neural network and then combines these estimates with geometric constraints provided by a 2D object bounding box to produce a complete 3D bounding box. The first network output estimates the 3D object orientation using a novel hybrid discrete-continuous loss, which significantly outperforms the L2 loss. The second output regresses the 3D object dimensions, which have relatively little variance compared to alternatives and can often be predicted for many object types. These estimates, combined with the geometric constraints on translation imposed by the 2D bounding box, enable us to recover a stable and accurate 3D object pose. We evaluate our method on the challenging KITTI object detection benchmark both on the official metric of 3D orientation estimation and also on the accuracy of the obtained 3D bounding boxes. Although conceptually simple, our method outperforms more complex and computationally expensive approaches that leverage semantic segmentation, instance level segmentation and flat ground priors and sub-category detection. Our discrete-continuous loss also produces state of the art results for 3D viewpoint estimation on the Pascal 3D+ dataset.},
  archiveprefix = {arXiv},
  eprint = {1612.00496},
  eprinttype = {arxiv},
  file = {/Users/anselm/Zotero/storage/V4SRUKCS/Mousavian et al. - 2017 - 3D Bounding Box Estimation Using Deep Learning and.pdf;/Users/anselm/Zotero/storage/VJR2NBDY/1612.html},
  journal = {arXiv:1612.00496 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  primaryclass = {cs}
}

@article{mur-artalORBSLAM2OpenSourceSLAM2016,
  title = {{{ORB}}-{{SLAM2}}: An {{Open}}-{{Source SLAM System}} for {{Monocular}}, {{Stereo}} and {{RGB}}-{{D Cameras}}},
  shorttitle = {{{ORB}}-{{SLAM2}}},
  author = {{Mur-Artal}, Raul and Tardos, Juan D.},
  year = {2016},
  month = oct,
  doi = {10.1109/TRO.2017.2705103},
  abstract = {We present ORB-SLAM2 a complete SLAM system for monocular, stereo and RGB-D cameras, including map reuse, loop closing and relocalization capabilities. The system works in real-time on standard CPUs in a wide variety of environments from small hand-held indoors sequences, to drones flying in industrial environments and cars driving around a city. Our back-end based on bundle adjustment with monocular and stereo observations allows for accurate trajectory estimation with metric scale. Our system includes a lightweight localization mode that leverages visual odometry tracks for unmapped regions and matches to map points that allow for zero-drift localization. The evaluation on 29 popular public sequences shows that our method achieves state-of-the-art accuracy, being in most cases the most accurate SLAM solution. We publish the source code, not only for the benefit of the SLAM community, but with the aim of being an out-of-the-box SLAM solution for researchers in other fields.},
  file = {/Users/anselm/Zotero/storage/ZAGXQBGF/Mur-Artal and Tardos - 2016 - ORB-SLAM2 an Open-Source SLAM System for Monocula.pdf;/Users/anselm/Zotero/storage/HTDSBRP5/1610.html},
  language = {en}
}

@article{mur-artalORBSLAM2OpenSourceSLAM2017,
  title = {{{ORB}}-{{SLAM2}}: An {{Open}}-{{Source SLAM System}} for {{Monocular}}, {{Stereo}} and {{RGB}}-{{D Cameras}}},
  shorttitle = {{{ORB}}-{{SLAM2}}},
  author = {{Mur-Artal}, Raul and Tardos, Juan D.},
  year = {2017},
  month = oct,
  volume = {33},
  pages = {1255--1262},
  issn = {1552-3098, 1941-0468},
  doi = {10.1109/TRO.2017.2705103},
  abstract = {We present ORB-SLAM2 a complete SLAM system for monocular, stereo and RGB-D cameras, including map reuse, loop closing and relocalization capabilities. The system works in real-time on standard CPUs in a wide variety of environments from small hand-held indoors sequences, to drones flying in industrial environments and cars driving around a city. Our back-end based on bundle adjustment with monocular and stereo observations allows for accurate trajectory estimation with metric scale. Our system includes a lightweight localization mode that leverages visual odometry tracks for unmapped regions and matches to map points that allow for zero-drift localization. The evaluation on 29 popular public sequences shows that our method achieves state-of-the-art accuracy, being in most cases the most accurate SLAM solution. We publish the source code, not only for the benefit of the SLAM community, but with the aim of being an out-of-the-box SLAM solution for researchers in other fields.},
  archiveprefix = {arXiv},
  eprint = {1610.06475},
  eprinttype = {arxiv},
  file = {/Users/anselm/Zotero/storage/DHZZ27BB/Mur-Artal and Tardos - 2017 - ORB-SLAM2 an Open-Source SLAM System for Monocula.pdf;/Users/anselm/Zotero/storage/WWDYIYT2/1610.html},
  journal = {IEEE Transactions on Robotics},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Robotics},
  number = {5}
}

@article{mur-artalORBSLAMVersatileAccurate2015,
  title = {{{ORB}}-{{SLAM}}: A {{Versatile}} and {{Accurate Monocular SLAM System}}},
  shorttitle = {{{ORB}}-{{SLAM}}},
  author = {{Mur-Artal}, Raul and Montiel, J. M. M. and Tardos, Juan D.},
  year = {2015},
  month = feb,
  doi = {10.1109/TRO.2015.2463671},
  abstract = {This paper presents ORB-SLAM, a feature-based monocular SLAM system that operates in real time, in small and large, indoor and outdoor environments. The system is robust to severe motion clutter, allows wide baseline loop closing and relocalization, and includes full automatic initialization. Building on excellent algorithms of recent years, we designed from scratch a novel system that uses the same features for all SLAM tasks: tracking, mapping, relocalization, and loop closing. A survival of the fittest strategy that selects the points and keyframes of the reconstruction leads to excellent robustness and generates a compact and trackable map that only grows if the scene content changes, allowing lifelong operation. We present an exhaustive evaluation in 27 sequences from the most popular datasets. ORB-SLAM achieves unprecedented performance with respect to other state-of-the-art monocular SLAM approaches. For the benefit of the community, we make the source code public.},
  file = {/Users/anselm/Zotero/storage/FZZRSBVQ/Mur-Artal et al. - 2015 - ORB-SLAM a Versatile and Accurate Monocular SLAM .pdf;/Users/anselm/Zotero/storage/QZN85YQP/1502.html},
  language = {en}
}

@article{mur-artalORBSLAMVersatileAccurate2015a,
  title = {{{ORB}}-{{SLAM}}: A {{Versatile}} and {{Accurate Monocular SLAM System}}},
  shorttitle = {{{ORB}}-{{SLAM}}},
  author = {{Mur-Artal}, Raul and Montiel, J. M. M. and Tardos, Juan D.},
  year = {2015},
  month = oct,
  volume = {31},
  pages = {1147--1163},
  issn = {1552-3098, 1941-0468},
  doi = {10.1109/TRO.2015.2463671},
  abstract = {This paper presents ORB-SLAM, a feature-based monocular SLAM system that operates in real time, in small and large, indoor and outdoor environments. The system is robust to severe motion clutter, allows wide baseline loop closing and relocalization, and includes full automatic initialization. Building on excellent algorithms of recent years, we designed from scratch a novel system that uses the same features for all SLAM tasks: tracking, mapping, relocalization, and loop closing. A survival of the fittest strategy that selects the points and keyframes of the reconstruction leads to excellent robustness and generates a compact and trackable map that only grows if the scene content changes, allowing lifelong operation. We present an exhaustive evaluation in 27 sequences from the most popular datasets. ORB-SLAM achieves unprecedented performance with respect to other state-of-the-art monocular SLAM approaches. For the benefit of the community, we make the source code public.},
  archiveprefix = {arXiv},
  eprint = {1502.00956},
  eprinttype = {arxiv},
  file = {/Users/anselm/Zotero/storage/V6NE5LFF/Mur-Artal et al. - 2015 - ORB-SLAM a Versatile and Accurate Monocular SLAM .pdf;/Users/anselm/Zotero/storage/IMZ8JVVC/1502.html},
  journal = {IEEE Transactions on Robotics},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Robotics},
  number = {5}
}

@article{muraliUtilizingSemanticVisual2018,
  title = {Utilizing {{Semantic Visual Landmarks}} for {{Precise Vehicle Navigation}}},
  author = {Murali, Varun and Chiu, Han-Pang and Samarasekera, Supun and Rakesh and Kumar},
  year = {2018},
  month = jan,
  abstract = {This paper presents a new approach for integrating semantic information for vision-based vehicle navigation. Although vision-based vehicle navigation systems using premapped visual landmarks are capable of achieving submeter level accuracy in large-scale urban environment, a typical error source in this type of systems comes from the presence of visual landmarks or features from temporal objects in the environment, such as cars and pedestrians. We propose a gated factor graph framework to use semantic information associated with visual features to make decisions on outlier/ inlier computation from three perspectives: the feature tracking process, the geo-referenced map building process, and the navigation system using pre-mapped landmarks. The class category that the visual feature belongs to is extracted from a pre-trained deep learning network trained for semantic segmentation. The feasibility and generality of our approach is demonstrated by our implementations on top of two vision-based navigation systems. Experimental evaluations validate that the injection of semantic information associated with visual landmarks using our approach achieves substantial improvements in accuracy on GPS-denied navigation solutions for large-scale urban scenarios.},
  archiveprefix = {arXiv},
  eprint = {1801.00858},
  eprinttype = {arxiv},
  file = {/Users/anselm/Zotero/storage/TYV6EUUZ/Murali et al. - 2018 - Utilizing Semantic Visual Landmarks for Precise Ve.pdf},
  journal = {arXiv:1801.00858 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  language = {en},
  primaryclass = {cs}
}

@book{murrayMathematicalIntroductionRobotic1994,
  title = {A Mathematical Introduction to Robotic Manipulation},
  author = {Murray, Richard M. and Li, Zexiang and Sastry, Shankar},
  year = {1994},
  publisher = {{CRC Press}},
  address = {{Boca Raton}},
  isbn = {978-0-8493-7981-9},
  keywords = {Robotics},
  lccn = {TJ211 .M87 1994}
}

@inproceedings{NIPS2015_6da37dd3,
  title = {{{3D}} Object Proposals for Accurate Object Class Detection},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Chen, Xiaozhi and Kundu, Kaustav and Zhu, Yukun and Berneshawi, Andrew G and Ma, Huimin and Fidler, Sanja and Urtasun, Raquel},
  editor = {Cortes, C. and Lawrence, N. and Lee, D. and Sugiyama, M. and Garnett, R.},
  year = {2015},
  volume = {28},
  publisher = {{Curran Associates, Inc.}}
}

@article{osepCombinedImageWorldSpace2018,
  title = {Combined {{Image}}- and {{World}}-{{Space Tracking}} in {{Traffic Scenes}}},
  author = {Osep, Aljosa and Mehner, Wolfgang and Mathias, Markus and Leibe, Bastian},
  year = {2018},
  month = sep,
  abstract = {Tracking in urban street scenes plays a central role in autonomous systems such as self-driving cars. Most of the current vision-based tracking methods perform tracking in the image domain. Other approaches, e.g. based on LIDAR and radar, track purely in 3D. While some vision-based tracking methods invoke 3D information in parts of their pipeline, and some 3D-based methods utilize image-based information in components of their approach, we propose to use image- and world-space information jointly throughout our method. We present our tracking pipeline as a 3D extension of image-based tracking. From enhancing the detections with 3D measurements to the reported positions of every tracked object, we use worldspace 3D information at every stage of processing. We accomplish this by our novel coupled 2D-3D Kalman filter, combined with a conceptually clean and extendable hypothesize-andselect framework. Our approach matches the current stateof-the-art on the official KITTI benchmark, which performs evaluation in the 2D image domain only. Further experiments show significant improvements in 3D localization precision by enabling our coupled 2D-3D tracking.},
  archiveprefix = {arXiv},
  eprint = {1809.07357},
  eprinttype = {arxiv},
  file = {/Users/anselm/Zotero/storage/7NMXILFP/Osep et al. - 2018 - Combined Image- and World-Space Tracking in Traffi.pdf},
  journal = {arXiv:1809.07357 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  language = {en},
  primaryclass = {cs}
}

@inproceedings{osepMultiscaleObjectCandidates2016,
  title = {Multi-Scale Object Candidates for Generic Object Tracking in Street Scenes},
  booktitle = {2016 {{IEEE International Conference}} on {{Robotics}} and {{Automation}} ({{ICRA}})},
  author = {Osep, Aljosa and Hermans, Alexander and Engelmann, Francis and Klostermann, Dirk and Mathias, Markus and Leibe, Bastian},
  year = {2016},
  month = may,
  pages = {3180--3187},
  publisher = {{IEEE}},
  address = {{Stockholm}},
  doi = {10.1109/ICRA.2016.7487487},
  abstract = {Most vision based systems for object tracking in urban environments focus on a limited number of important object categories such as cars or pedestrians, for which powerful detectors are available. However, practical driving scenarios contain many additional objects of interest, for which suitable detectors either do not yet exist or would be cumbersome to obtain. In this paper we propose a more general tracking approach which does not follow the often used tracking-bydetection principle. Instead, we investigate how far we can get by tracking unknown, generic objects in challenging street scenes. As such, we do not restrict ourselves to only tracking the most common categories, but are able to handle a large variety of static and moving objects. We evaluate our approach on the KITTI dataset and show competitive results for the annotated classes, even though we are not restricted to them.},
  file = {/Users/anselm/Zotero/storage/7TLI3W89/Osep et al. - 2016 - Multi-scale object candidates for generic object t.pdf},
  isbn = {978-1-4673-8026-3},
  language = {en}
}

@book{papert1966summer,
  title = {The Summer Vision Project},
  author = {Papert, S.},
  year = {1966},
  publisher = {{Massachusetts Institute of Technology, Project MAC}},
  series = {{{AI}} Memo}
}

@article{qiFrustumPointNets3D2018,
  title = {Frustum {{PointNets}} for {{3D Object Detection}} from {{RGB}}-{{D Data}}},
  author = {Qi, Charles R. and Liu, Wei and Wu, Chenxia and Su, Hao and Guibas, Leonidas J.},
  year = {2018},
  month = apr,
  abstract = {In this work, we study 3D object detection from RGBD data in both indoor and outdoor scenes. While previous methods focus on images or 3D voxels, often obscuring natural 3D patterns and invariances of 3D data, we directly operate on raw point clouds by popping up RGB-D scans. However, a key challenge of this approach is how to efficiently localize objects in point clouds of large-scale scenes (region proposal). Instead of solely relying on 3D proposals, our method leverages both mature 2D object detectors and advanced 3D deep learning for object localization, achieving efficiency as well as high recall for even small objects. Benefited from learning directly in raw point clouds, our method is also able to precisely estimate 3D bounding boxes even under strong occlusion or with very sparse points. Evaluated on KITTI and SUN RGB-D 3D detection benchmarks, our method outperforms the state of the art by remarkable margins while having real-time capability.},
  archiveprefix = {arXiv},
  eprint = {1711.08488},
  eprinttype = {arxiv},
  file = {/Users/anselm/Zotero/storage/PY25UUXW/Qi et al. - 2018 - Frustum PointNets for 3D Object Detection from RGB.pdf},
  journal = {arXiv:1711.08488 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  language = {en},
  primaryclass = {cs}
}

@inproceedings{reddyCarFusionCombiningPoint2018,
  title = {{{CarFusion}}: {{Combining Point Tracking}} and {{Part Detection}} for {{Dynamic 3D Reconstruction}} of {{Vehicles}}},
  shorttitle = {{{CarFusion}}},
  booktitle = {2018 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Reddy, N Dinesh and Vo, Minh and Narasimhan, Srinivasa G.},
  year = {2018},
  month = jun,
  pages = {1906--1915},
  publisher = {{IEEE}},
  address = {{Salt Lake City, UT, USA}},
  doi = {10.1109/CVPR.2018.00204},
  isbn = {978-1-5386-6420-9}
}

@article{redmonYouOnlyLook2016,
  title = {You {{Only Look Once}}: {{Unified}}, {{Real}}-{{Time Object Detection}}},
  shorttitle = {You {{Only Look Once}}},
  author = {Redmon, Joseph and Divvala, Santosh and Girshick, Ross and Farhadi, Ali},
  year = {2016},
  month = may,
  abstract = {We present YOLO, a new approach to object detection. Prior work on object detection repurposes classifiers to perform detection. Instead, we frame object detection as a regression problem to spatially separated bounding boxes and associated class probabilities. A single neural network predicts bounding boxes and class probabilities directly from full images in one evaluation. Since the whole detection pipeline is a single network, it can be optimized end-to-end directly on detection performance. Our unified architecture is extremely fast. Our base YOLO model processes images in real-time at 45 frames per second. A smaller version of the network, Fast YOLO, processes an astounding 155 frames per second while still achieving double the mAP of other real-time detectors. Compared to state-of-the-art detection systems, YOLO makes more localization errors but is far less likely to predict false detections where nothing exists. Finally, YOLO learns very general representations of objects. It outperforms all other detection methods, including DPM and R-CNN, by a wide margin when generalizing from natural images to artwork on both the Picasso Dataset and the People-Art Dataset.},
  archiveprefix = {arXiv},
  eprint = {1506.02640},
  eprinttype = {arxiv},
  file = {/Users/anselm/Zotero/storage/9RBYYN5V/Redmon et al. - 2016 - You Only Look Once Unified, Real-Time Object Dete.pdf;/Users/anselm/Zotero/storage/T8KEDEUT/1506.html},
  journal = {arXiv:1506.02640 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  primaryclass = {cs}
}

@article{redmonYouOnlyLook2016a,
  title = {You {{Only Look Once}}: {{Unified}}, {{Real}}-{{Time Object Detection}}},
  shorttitle = {You {{Only Look Once}}},
  author = {Redmon, Joseph and Divvala, Santosh and Girshick, Ross and Farhadi, Ali},
  year = {2016},
  month = may,
  abstract = {We present YOLO, a new approach to object detection. Prior work on object detection repurposes classifiers to perform detection. Instead, we frame object detection as a regression problem to spatially separated bounding boxes and associated class probabilities. A single neural network predicts bounding boxes and class probabilities directly from full images in one evaluation. Since the whole detection pipeline is a single network, it can be optimized end-to-end directly on detection performance. Our unified architecture is extremely fast. Our base YOLO model processes images in real-time at 45 frames per second. A smaller version of the network, Fast YOLO, processes an astounding 155 frames per second while still achieving double the mAP of other real-time detectors. Compared to state-of-the-art detection systems, YOLO makes more localization errors but is far less likely to predict false detections where nothing exists. Finally, YOLO learns very general representations of objects. It outperforms all other detection methods, including DPM and R-CNN, by a wide margin when generalizing from natural images to artwork on both the Picasso Dataset and the People-Art Dataset.},
  archiveprefix = {arXiv},
  eprint = {1506.02640},
  eprinttype = {arxiv},
  file = {/Users/anselm/Zotero/storage/EGV622D8/Redmon et al. - 2016 - You Only Look Once Unified, Real-Time Object Dete.pdf;/Users/anselm/Zotero/storage/TS8ZS4RG/1506.html},
  journal = {arXiv:1506.02640 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  primaryclass = {cs}
}

@article{renAccurateSingleStage2017,
  title = {Accurate {{Single Stage Detector Using Recurrent Rolling Convolution}}},
  author = {Ren, Jimmy and Chen, Xiaohao and Liu, Jianbo and Sun, Wenxiu and Pang, Jiahao and Yan, Qiong and Tai, Yu-Wing and Xu, Li},
  year = {2017},
  month = apr,
  abstract = {Most of the recent successful methods in accurate object detection and localization used some variants of R-CNN style two stage Convolutional Neural Networks (CNN) where plausible regions were proposed in the first stage then followed by a second stage for decision refinement. Despite the simplicity of training and the efficiency in deployment, the single stage detection methods have not been as competitive when evaluated in benchmarks consider mAP for high IoU thresholds. In this paper, we proposed a novel single stage end-to-end trainable object detection network to overcome this limitation. We achieved this by introducing Recurrent Rolling Convolution (RRC) architecture over multi-scale feature maps to construct object classifiers and bounding box regressors which are "deep in context". We evaluated our method in the challenging KITTI dataset which measures methods under IoU threshold of 0.7. We showed that with RRC, a single reduced VGG-16 based model already significantly outperformed all the previously published results. At the time this paper was written our models ranked the first in KITTI car detection (the hard level), the first in cyclist detection and the second in pedestrian detection. These results were not reached by the previous single stage methods. The code is publicly available.},
  archiveprefix = {arXiv},
  eprint = {1704.05776},
  eprinttype = {arxiv},
  file = {/Users/anselm/Zotero/storage/BIAG5RRD/Ren et al. - 2017 - Accurate Single Stage Detector Using Recurrent Rol.pdf;/Users/anselm/Zotero/storage/K4GXTPV5/1704.html},
  journal = {arXiv:1704.05776 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  primaryclass = {cs}
}

@inproceedings{renckenConcurrentLocalisationMap1993,
  title = {Concurrent Localisation and Map Building for Mobile Robots Using Ultrasonic Sensors},
  booktitle = {Proceedings of 1993 {{IEEE}}/{{RSJ International Conference}} on {{Intelligent Robots}} and {{Systems}} ({{IROS}} '93)},
  author = {Rencken, W.D.},
  year = {1993},
  volume = {3},
  pages = {2192--2197},
  publisher = {{IEEE}},
  address = {{Yokohama, Japan}},
  doi = {10.1109/IROS.1993.583932},
  isbn = {978-0-7803-0823-7}
}

@article{renFasterRCNNRealTime2016,
  title = {Faster {{R}}-{{CNN}}: {{Towards Real}}-{{Time Object Detection}} with {{Region Proposal Networks}}},
  shorttitle = {Faster {{R}}-{{CNN}}},
  author = {Ren, Shaoqing and He, Kaiming and Girshick, Ross and Sun, Jian},
  year = {2016},
  month = jan,
  abstract = {State-of-the-art object detection networks depend on region proposal algorithms to hypothesize object locations. Advances like SPPnet and Fast R-CNN have reduced the running time of these detection networks, exposing region proposal computation as a bottleneck. In this work, we introduce a Region Proposal Network (RPN) that shares full-image convolutional features with the detection network, thus enabling nearly cost-free region proposals. An RPN is a fully convolutional network that simultaneously predicts object bounds and objectness scores at each position. The RPN is trained end-to-end to generate high-quality region proposals, which are used by Fast R-CNN for detection. We further merge RPN and Fast R-CNN into a single network by sharing their convolutional features---using the recently popular terminology of neural networks with 'attention' mechanisms, the RPN component tells the unified network where to look. For the very deep VGG-16 model, our detection system has a frame rate of 5fps (including all steps) on a GPU, while achieving state-of-the-art object detection accuracy on PASCAL VOC 2007, 2012, and MS COCO datasets with only 300 proposals per image. In ILSVRC and COCO 2015 competitions, Faster R-CNN and RPN are the foundations of the 1st-place winning entries in several tracks. Code has been made publicly available.},
  archiveprefix = {arXiv},
  eprint = {1506.01497},
  eprinttype = {arxiv},
  file = {/Users/anselm/Zotero/storage/AHKGDGHF/Ren et al. - 2016 - Faster R-CNN Towards Real-Time Object Detection w.pdf;/Users/anselm/Zotero/storage/MH535NFX/1506.html},
  journal = {arXiv:1506.01497 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  primaryclass = {cs}
}

@article{renFasterRCNNRealTime2017,
  title = {Faster {{R}}-{{CNN}}: {{Towards Real}}-{{Time Object Detection}} with {{Region Proposal Networks}}},
  shorttitle = {Faster {{R}}-{{CNN}}},
  author = {Ren, Shaoqing and He, Kaiming and Girshick, Ross and Sun, Jian},
  year = {2017},
  month = jun,
  volume = {39},
  pages = {1137--1149},
  issn = {0162-8828, 2160-9292},
  doi = {10.1109/TPAMI.2016.2577031},
  file = {/Users/anselm/Zotero/storage/5QKEAKGS/Ren et al. - 2017 - Faster R-CNN Towards Real-Time Object Detection w.pdf},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number = {6}
}

@article{rezatofighiGeneralizedIntersectionUnion2019,
  title = {Generalized {{Intersection}} over {{Union}}: {{A Metric}} and {{A Loss}} for {{Bounding Box Regression}}},
  shorttitle = {Generalized {{Intersection}} over {{Union}}},
  author = {Rezatofighi, Hamid and Tsoi, Nathan and Gwak, JunYoung and Sadeghian, Amir and Reid, Ian and Savarese, Silvio},
  year = {2019},
  month = apr,
  abstract = {Intersection over Union (IoU) is the most popular evaluation metric used in the object detection benchmarks. However, there is a gap between optimizing the commonly used distance losses for regressing the parameters of a bounding box and maximizing this metric value. The optimal objective for a metric is the metric itself. In the case of axis-aligned 2D bounding boxes, it can be shown that \$IoU\$ can be directly used as a regression loss. However, \$IoU\$ has a plateau making it infeasible to optimize in the case of non-overlapping bounding boxes. In this paper, we address the weaknesses of \$IoU\$ by introducing a generalized version as both a new loss and a new metric. By incorporating this generalized \$IoU\$ (\$GIoU\$) as a loss into the state-of-the art object detection frameworks, we show a consistent improvement on their performance using both the standard, \$IoU\$ based, and new, \$GIoU\$ based, performance measures on popular object detection benchmarks such as PASCAL VOC and MS COCO.},
  archiveprefix = {arXiv},
  eprint = {1902.09630},
  eprinttype = {arxiv},
  file = {/Users/anselm/Zotero/storage/LWMRS6JF/Rezatofighi et al. - 2019 - Generalized Intersection over Union A Metric and .pdf;/Users/anselm/Zotero/storage/62UJXAXB/1902.html},
  journal = {arXiv:1902.09630 [cs]},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  primaryclass = {cs}
}

@inproceedings{riazueloSemanticVisualSLAM2017,
  title = {Semantic Visual {{SLAM}} in Populated Environments},
  booktitle = {2017 {{European Conference}} on {{Mobile Robots}} ({{ECMR}})},
  author = {Riazuelo, L. and Montano, L. and Montiel, J. M. M.},
  year = {2017},
  month = sep,
  pages = {1--7},
  publisher = {{IEEE}},
  address = {{Paris}},
  doi = {10.1109/ECMR.2017.8098697},
  isbn = {978-1-5386-1096-1}
}

@article{ristaniPerformanceMeasuresData,
  title = {Performance {{Measures}} and a {{Data Set}} for {{Multi}}-{{Target}}, {{Multi}}-{{Camera Tracking}}},
  author = {Ristani, Ergys and Solera, Francesco and Zou, Roger and Cucchiara, Rita and Tomasi, Carlo},
  pages = {18},
  abstract = {To help accelerate progress in multi-target, multi-camera tracking systems, we present (i) a new pair of precision-recall measures of performance that treats errors of all types uniformly and emphasizes correct identification over sources of error; (ii) the largest fully-annotated and calibrated data set to date with more than 2 million frames of 1080p, 60fps video taken by 8 cameras observing more than 2,700 identities over 85 minutes; and (iii) a reference software system as a comparison baseline. We show that (i) our measures properly account for bottom-line identity match performance in the multi-camera setting; (ii) our data set poses realistic challenges to current trackers; and (iii) the performance of our system is comparable to the state of the art.},
  file = {/Users/anselm/Zotero/storage/Z9SRYWDI/Ristani et al. - Performance Measures and a Data Set for Multi-Targ.pdf},
  language = {en}
}

@article{ristaniPerformanceMeasuresData2016,
  title = {Performance {{Measures}} and a {{Data Set}} for {{Multi}}-{{Target}}, {{Multi}}-{{Camera Tracking}}},
  author = {Ristani, Ergys and Solera, Francesco and Zou, Roger S. and Cucchiara, Rita and Tomasi, Carlo},
  year = {2016},
  month = sep,
  abstract = {To help accelerate progress in multi-target, multi-camera tracking systems, we present (i) a new pair of precision-recall measures of performance that treats errors of all types uniformly and emphasizes correct identification over sources of error; (ii) the largest fully-annotated and calibrated data set to date with more than 2 million frames of 1080p, 60fps video taken by 8 cameras observing more than 2,700 identities over 85 minutes; and (iii) a reference software system as a comparison baseline. We show that (i) our measures properly account for bottom-line identity match performance in the multi-camera setting; (ii) our data set poses realistic challenges to current trackers; and (iii) the performance of our system is comparable to the state of the art.},
  archiveprefix = {arXiv},
  eprint = {1609.01775},
  eprinttype = {arxiv},
  file = {/Users/anselm/Zotero/storage/PJS4ZEWN/Ristani et al. - 2016 - Performance Measures and a Data Set for Multi-Targ.pdf;/Users/anselm/Zotero/storage/HJDVLUC2/1609.html},
  journal = {arXiv:1609.01775 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  primaryclass = {cs}
}

@incollection{rostenMachineLearningHighSpeed2006,
  title = {Machine {{Learning}} for {{High}}-{{Speed Corner Detection}}},
  booktitle = {Computer {{Vision}} \textendash{} {{ECCV}} 2006},
  author = {Rosten, Edward and Drummond, Tom},
  editor = {Leonardis, Ale{\v s} and Bischof, Horst and Pinz, Axel},
  year = {2006},
  volume = {3951},
  pages = {430--443},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/11744023_34},
  isbn = {978-3-540-33832-1 978-3-540-33833-8},
  language = {en}
}

@article{rubino3DObjectLocalisation2017,
  title = {{{3D Object Localisation}} from {{Multi}}-View {{Image Detections}}},
  author = {Rubino, Cosimo and Crocco, Marco and Del Bue, Alessio},
  year = {2017},
  pages = {1--1},
  issn = {0162-8828, 2160-9292},
  doi = {10.1109/TPAMI.2017.2701373},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence}
}

@inproceedings{rubleeORBEfficientAlternative2011,
  title = {{{ORB}}: {{An}} Efficient Alternative to {{SIFT}} or {{SURF}}},
  shorttitle = {{{ORB}}},
  booktitle = {2011 {{International Conference}} on {{Computer Vision}}},
  author = {Rublee, Ethan and Rabaud, Vincent and Konolige, Kurt and Bradski, Gary},
  year = {2011},
  month = nov,
  pages = {2564--2571},
  publisher = {{IEEE}},
  address = {{Barcelona, Spain}},
  doi = {10.1109/ICCV.2011.6126544},
  abstract = {Feature matching is at the base of many computer vi\- sion problems, such as object recognition or structure from motion. Current methods rely on costly descriptors for de\- tection and matching. In this paper, we propose a very fast binary descriptor based on BRIEF, called ORB, which is rotation invariant and resistant to noise. We demonstrate through experiments how ORB is at two orders of magni\- tude faster than SIFT, while peiforming as well in many situations. The efficiency is tested on several real-world ap\- plications, including object detection and patch-tracking on a smart phone.},
  file = {/Users/anselm/Zotero/storage/73RUB9B5/Rublee et al. - 2011 - ORB An efficient alternative to SIFT or SURF.pdf},
  isbn = {978-1-4577-1102-2 978-1-4577-1101-5 978-1-4577-1100-8},
  language = {en}
}

@article{russakovskyImageNetLargeScale2015,
  title = {{{ImageNet Large Scale Visual Recognition Challenge}}},
  author = {Russakovsky, Olga and Deng, Jia and Su, Hao and Krause, Jonathan and Satheesh, Sanjeev and Ma, Sean and Huang, Zhiheng and Karpathy, Andrej and Khosla, Aditya and Bernstein, Michael and Berg, Alexander C. and {Fei-Fei}, Li},
  year = {2015},
  month = jan,
  abstract = {The ImageNet Large Scale Visual Recognition Challenge is a benchmark in object category classification and detection on hundreds of object categories and millions of images. The challenge has been run annually from 2010 to present, attracting participation from more than fifty institutions. This paper describes the creation of this benchmark dataset and the advances in object recognition that have been possible as a result. We discuss the challenges of collecting large-scale ground truth annotation, highlight key breakthroughs in categorical object recognition, provide a detailed analysis of the current state of the field of large-scale image classification and object detection, and compare the state-of-the-art computer vision accuracy with human accuracy. We conclude with lessons learned in the five years of the challenge, and propose future directions and improvements.},
  archiveprefix = {arXiv},
  eprint = {1409.0575},
  eprinttype = {arxiv},
  file = {/Users/anselm/Zotero/storage/6WC4GYMY/Russakovsky et al. - 2015 - ImageNet Large Scale Visual Recognition Challenge.pdf;/Users/anselm/Zotero/storage/2LSMGHH6/1409.html},
  journal = {arXiv:1409.0575 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,I.4.8,I.5.2},
  primaryclass = {cs}
}

@inproceedings{salas-morenoSLAMSimultaneousLocalisation2013,
  title = {{{SLAM}}++: {{Simultaneous Localisation}} and {{Mapping}} at the {{Level}} of {{Objects}}},
  shorttitle = {{{SLAM}}++},
  booktitle = {2013 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {{Salas-Moreno}, Renato F. and Newcombe, Richard A. and Strasdat, Hauke and Kelly, Paul H.J. and Davison, Andrew J.},
  year = {2013},
  month = jun,
  pages = {1352--1359},
  publisher = {{IEEE}},
  address = {{Portland, OR, USA}},
  doi = {10.1109/CVPR.2013.178},
  abstract = {We present the major advantages of a new `object oriented' 3D SLAM paradigm, which takes full advantage in the loop of prior knowledge that many scenes consist of repeated, domain-specific objects and structures. As a hand-held depth camera browses a cluttered scene, realtime 3D object recognition and tracking provides 6DoF camera-object constraints which feed into an explicit graph of objects, continually refined by efficient pose-graph optimisation. This offers the descriptive and predictive power of SLAM systems which perform dense surface reconstruction, but with a huge representation compression. The object graph enables predictions for accurate ICP-based camera to model tracking at each live frame, and efficient active search for new objects in currently undescribed image regions. We demonstrate real-time incremental SLAM in large, cluttered environments, including loop closure, relocalisation and the detection of moved objects, and of course the generation of an object level scene description with the potential to enable interaction.},
  file = {/Users/anselm/Zotero/storage/ALEPL6JF/Salas-Moreno et al. - 2013 - SLAM++ Simultaneous Localisation and Mapping at t.pdf},
  isbn = {978-0-7695-4989-7},
  language = {en}
}

@article{sarlinSuperGlueLearningFeature2020,
  title = {{{SuperGlue}}: {{Learning Feature Matching}} with {{Graph Neural Networks}}},
  shorttitle = {{{SuperGlue}}},
  author = {Sarlin, Paul-Edouard and DeTone, Daniel and Malisiewicz, Tomasz and Rabinovich, Andrew},
  year = {2020},
  month = mar,
  abstract = {This paper introduces SuperGlue, a neural network that matches two sets of local features by jointly finding correspondences and rejecting non-matchable points. Assignments are estimated by solving a differentiable optimal transport problem, whose costs are predicted by a graph neural network. We introduce a flexible context aggregation mechanism based on attention, enabling SuperGlue to reason about the underlying 3D scene and feature assignments jointly. Compared to traditional, hand-designed heuristics, our technique learns priors over geometric transformations and regularities of the 3D world through end-to-end training from image pairs. SuperGlue outperforms other learned approaches and achieves state-of-the-art results on the task of pose estimation in challenging real-world indoor and outdoor environments. The proposed method performs matching in real-time on a modern GPU and can be readily integrated into modern SfM or SLAM systems. The code and trained weights are publicly available at github.com/magicleap/SuperGluePretrainedNetwork.},
  archiveprefix = {arXiv},
  eprint = {1911.11763},
  eprinttype = {arxiv},
  file = {/Users/anselm/Zotero/storage/XC55B3VH/Sarlin et al. - 2020 - SuperGlue Learning Feature Matching with Graph Ne.pdf},
  journal = {arXiv:1911.11763 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  language = {en},
  primaryclass = {cs}
}

@book{schantzHistoryOCROptical1982,
  title = {The History of {{OCR}}, Optical Character Recognition},
  author = {Schantz, Herbert F.},
  year = {1982},
  publisher = {{Recognition Technologies Users Association}},
  address = {{Manchester Center, Vt.}},
  isbn = {978-0-943072-01-2},
  keywords = {History,Optical character recognition devices},
  lccn = {TA1640 .S33 1982}
}

@inproceedings{schulterDeepNetworkFlow2017,
  title = {Deep {{Network Flow}} for {{Multi}}-Object {{Tracking}}},
  booktitle = {2017 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Schulter, Samuel and Vernaza, Paul and Choi, Wongun and Chandraker, Manmohan},
  year = {2017},
  month = jul,
  pages = {2730--2739},
  publisher = {{IEEE}},
  address = {{Honolulu, HI}},
  doi = {10.1109/CVPR.2017.292},
  abstract = {Data association problems are an important component of many computer vision applications, with multi-object tracking being one of the most prominent examples. A typical approach to data association involves finding a graph matching or network flow that minimizes a sum of pairwise association costs, which are often either hand-crafted or learned as linear functions of fixed features. In this work, we demonstrate that it is possible to learn features for network-flow-based data association via backpropagation, by expressing the optimum of a smoothed network flow problem as a differentiable function of the pairwise association costs. We apply this approach to multi-object tracking with a network flow formulation. Our experiments demonstrate that we are able to successfully learn all cost functions for the association problem in an end-to-end fashion, which outperform hand-crafted costs in all settings. The integration and combination of various sources of inputs becomes easy and the cost functions can be learned entirely from data, alleviating tedious hand-designing of costs.},
  file = {/Users/anselm/Zotero/storage/KQLEVRYU/Schulter et al. - 2017 - Deep Network Flow for Multi-object Tracking.pdf},
  isbn = {978-1-5386-0457-1},
  language = {en}
}

@book{serraImageAnalysisMathematical1982,
  title = {Image Analysis and Mathematical Morphology},
  author = {Serra, Jean Paul},
  year = {1982},
  publisher = {{Academic Press}},
  address = {{London ; New York}},
  isbn = {978-0-12-637240-3},
  keywords = {Image processing},
  lccn = {TA1632 .S48 1982}
}

@article{sharmaPixelsLeveragingGeometry2018,
  title = {Beyond {{Pixels}}: {{Leveraging Geometry}} and {{Shape Cues}} for {{Online Multi}}-{{Object Tracking}}},
  shorttitle = {Beyond {{Pixels}}},
  author = {Sharma, Sarthak and Ansari, Junaid Ahmed and Murthy, J. Krishna and Krishna, K. Madhava},
  year = {2018},
  month = jul,
  abstract = {This paper introduces geometry and novel object shape and pose costs for multi-object tracking in road scenes. Using images from a monocular camera alone, we devise pairwise costs for object tracks, based on several 3D cues such as object pose, shape, and motion. The proposed costs are agnostic to the data association method and can be incorporated into any optimization framework to output the pairwise data associations. These costs are easy to implement, can be computed in real-time, and complement each other to account for possible errors in a tracking-by-detection framework. We perform an extensive analysis of the designed costs and empirically demonstrate consistent improvement over the state-of-the-art under varying conditions that employ a range of object detectors, exhibit a variety in camera and object motions, and, more importantly, are not reliant on the choice of the association framework. We also show that, by using the simplest of associations frameworks (two-frame Hungarian assignment), we surpass the state-of-the-art in multi-object-tracking on road scenes. More qualitative and quantitative results can be found at https://junaidcs032.github.io/Geometry\_ ObjectShape\_MOT/. Code and data to reproduce our experiments and results are now available at https://github. com/JunaidCS032/MOTBeyondPixels.},
  archiveprefix = {arXiv},
  eprint = {1802.09298},
  eprinttype = {arxiv},
  file = {/Users/anselm/Zotero/storage/PDXFRFH4/Sharma et al. - 2018 - Beyond Pixels Leveraging Geometry and Shape Cues .pdf},
  journal = {arXiv:1802.09298 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Robotics},
  language = {en},
  primaryclass = {cs}
}

@article{shiPointRCNN3DObject2019a,
  title = {{{PointRCNN}}: {{3D Object Proposal Generation}} and {{Detection}} from {{Point Cloud}}},
  shorttitle = {{{PointRCNN}}},
  author = {Shi, Shaoshuai and Wang, Xiaogang and Li, Hongsheng},
  year = {2019},
  month = may,
  abstract = {In this paper, we propose PointRCNN for 3D object detection from raw point cloud. The whole framework is composed of two stages: stage-1 for the bottom-up 3D proposal generation and stage-2 for refining proposals in the canonical coordinates to obtain the final detection results. Instead of generating proposals from RGB image or projecting point cloud to bird's view or voxels as previous methods do, our stage-1 sub-network directly generates a small number of high-quality 3D proposals from point cloud in a bottom-up manner via segmenting the point cloud of the whole scene into foreground points and background. The stage-2 sub-network transforms the pooled points of each proposal to canonical coordinates to learn better local spatial features, which is combined with global semantic features of each point learned in stage-1 for accurate box refinement and confidence prediction. Extensive experiments on the 3D detection benchmark of KITTI dataset show that our proposed architecture outperforms state-of-the-art methods with remarkable margins by using only point cloud as input. The code is available at https://github.com/sshaoshuai/PointRCNN.},
  archiveprefix = {arXiv},
  eprint = {1812.04244},
  eprinttype = {arxiv},
  file = {/Users/anselm/Zotero/storage/Y4A5R5YM/Shi et al. - 2019 - PointRCNN 3D Object Proposal Generation and Detec.pdf},
  journal = {arXiv:1812.04244 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  language = {en},
  primaryclass = {cs}
}

@article{sirovichLowdimensionalProcedureCharacterization1987a,
  title = {Low-Dimensional Procedure for the Characterization of Human Faces},
  author = {Sirovich, L. and Kirby, M.},
  year = {1987},
  month = mar,
  volume = {4},
  pages = {519},
  issn = {1084-7529, 1520-8532},
  doi = {10.1364/JOSAA.4.000519},
  file = {/Users/anselm/Zotero/storage/LH2IJXGB/Sirovich and Kirby - 1987 - Low-dimensional procedure for the characterization.pdf},
  journal = {Journal of the Optical Society of America A},
  language = {en},
  number = {3}
}

@article{smithSUSANNewApproach1997,
  title = {{{SUSAN}} - a New Approach to Low Level Image Processing},
  author = {Smith, Stephen M. and Brady, J. Michael},
  year = {1997},
  volume = {23},
  pages = {45--78},
  issn = {09205691},
  doi = {10.1023/A:1007963824710},
  journal = {International Journal of Computer Vision},
  number = {1}
}

@misc{soilleMorphologicalImageAnalysis0000,
  title = {Morphological {{Image Analysis}}: {{Principles}} and {{Applications}}},
  shorttitle = {Morphological {{Image Analysis}}},
  author = {Soille, Pierre},
  year = {0000 uu},
  annotation = {OCLC: 1159708118},
  isbn = {9783662050880},
  language = {English}
}

@article{songSemanticSceneCompletion2016,
  title = {Semantic {{Scene Completion}} from a {{Single Depth Image}}},
  author = {Song, Shuran and Yu, Fisher and Zeng, Andy and Chang, Angel X. and Savva, Manolis and Funkhouser, Thomas},
  year = {2016},
  month = nov,
  abstract = {This paper focuses on semantic scene completion, a task for producing a complete 3D voxel representation of volumetric occupancy and semantic labels for a scene from a single-view depth map observation. Previous work has considered scene completion and semantic labeling of depth maps separately. However, we observe that these two problems are tightly intertwined. To leverage the coupled nature of these two tasks, we introduce the semantic scene completion network (SSCNet), an end-to-end 3D convolutional network that takes a single depth image as input and simultaneously outputs occupancy and semantic labels for all voxels in the camera view frustum. Our network uses a dilation-based 3D context module to efficiently expand the receptive field and enable 3D context learning. To train our network, we construct SUNCG - a manually created large-scale dataset of synthetic 3D scenes with dense volumetric annotations. Our experiments demonstrate that the joint model outperforms methods addressing each task in isolation and outperforms alternative approaches on the semantic scene completion task.},
  archiveprefix = {arXiv},
  eprint = {1611.08974},
  eprinttype = {arxiv},
  file = {/Users/anselm/Zotero/storage/2SXVGDCH/Song et al. - 2016 - Semantic Scene Completion from a Single Depth Imag.pdf;/Users/anselm/Zotero/storage/J4JIDB2J/1611.html},
  journal = {arXiv:1611.08974 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  primaryclass = {cs}
}

@inproceedings{songSUNRGBDRGBD2015,
  title = {{{SUN RGB}}-{{D}}: {{A RGB}}-{{D}} Scene Understanding Benchmark Suite},
  shorttitle = {{{SUN RGB}}-{{D}}},
  booktitle = {2015 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Song, Shuran and Lichtenberg, Samuel P. and Xiao, Jianxiong},
  year = {2015},
  month = jun,
  pages = {567--576},
  publisher = {{IEEE}},
  address = {{Boston, MA, USA}},
  doi = {10.1109/CVPR.2015.7298655},
  abstract = {Although RGB-D sensors have enabled major breakthroughs for several vision tasks, such as 3D reconstruction, we have not attained the same level of success in highlevel scene understanding. Perhaps one of the main reasons is the lack of a large-scale benchmark with 3D annotations and 3D evaluation metrics. In this paper, we introduce an RGB-D benchmark suite for the goal of advancing the state-of-the-arts in all major scene understanding tasks. Our dataset is captured by four different sensors and contains 10,335 RGB-D images, at a similar scale as PASCAL VOC. The whole dataset is densely annotated and includes 146,617 2D polygons and 64,595 3D bounding boxes with accurate object orientations, as well as a 3D room layout and scene category for each image. This dataset enables us to train data-hungry algorithms for scene-understanding tasks, evaluate them using meaningful 3D metrics, avoid overfitting to a small testing set, and study cross-sensor bias.},
  file = {/Users/anselm/Zotero/storage/55JYBT25/Song et al. - 2015 - SUN RGB-D A RGB-D scene understanding benchmark s.pdf},
  isbn = {978-1-4673-6964-0},
  language = {en}
}

@inproceedings{sturmBenchmarkEvaluationRGBD2012,
  title = {A Benchmark for the Evaluation of {{RGB}}-{{D SLAM}} Systems},
  booktitle = {2012 {{IEEE}}/{{RSJ International Conference}} on {{Intelligent Robots}} and {{Systems}}},
  author = {Sturm, Jrgen and Engelhard, Nikolas and Endres, Felix and Burgard, Wolfram and Cremers, Daniel},
  year = {2012},
  month = oct,
  pages = {573--580},
  publisher = {{IEEE}},
  address = {{Vilamoura-Algarve, Portugal}},
  doi = {10.1109/IROS.2012.6385773},
  isbn = {978-1-4673-1736-8 978-1-4673-1737-5 978-1-4673-1735-1}
}

@article{sunImprovingRGBDSLAM2017,
  title = {Improving {{RGB}}-{{D SLAM}} in Dynamic Environments: {{A}} Motion Removal Approach},
  shorttitle = {Improving {{RGB}}-{{D SLAM}} in Dynamic Environments},
  author = {Sun, Yuxiang and Liu, Ming and Meng, Max Q.-H.},
  year = {2017},
  month = mar,
  volume = {89},
  pages = {110--122},
  issn = {09218890},
  doi = {10.1016/j.robot.2016.11.012},
  journal = {Robotics and Autonomous Systems},
  language = {en}
}

@article{sunScalabilityPerceptionAutonomous2020,
  title = {Scalability in {{Perception}} for {{Autonomous Driving}}: {{Waymo Open Dataset}}},
  shorttitle = {Scalability in {{Perception}} for {{Autonomous Driving}}},
  author = {Sun, Pei and Kretzschmar, Henrik and Dotiwalla, Xerxes and Chouard, Aurelien and Patnaik, Vijaysai and Tsui, Paul and Guo, James and Zhou, Yin and Chai, Yuning and Caine, Benjamin and Vasudevan, Vijay and Han, Wei and Ngiam, Jiquan and Zhao, Hang and Timofeev, Aleksei and Ettinger, Scott and Krivokon, Maxim and Gao, Amy and Joshi, Aditya and Zhao, Sheng and Cheng, Shuyang and Zhang, Yu and Shlens, Jonathon and Chen, Zhifeng and Anguelov, Dragomir},
  year = {2020},
  month = may,
  abstract = {The research community has increasing interest in autonomous driving research, despite the resource intensity of obtaining representative real world data. Existing self-driving datasets are limited in the scale and variation of the environments they capture, even though generalization within and between operating regions is crucial to the overall viability of the technology. In an effort to help align the research community's contributions with real-world self-driving problems, we introduce a new large scale, high quality, diverse dataset. Our new dataset consists of 1150 scenes that each span 20 seconds, consisting of well synchronized and calibrated high quality LiDAR and camera data captured across a range of urban and suburban geographies. It is 15x more diverse than the largest camera+LiDAR dataset available based on our proposed diversity metric. We exhaustively annotated this data with 2D (camera image) and 3D (LiDAR) bounding boxes, with consistent identifiers across frames. Finally, we provide strong baselines for 2D as well as 3D detection and tracking tasks. We further study the effects of dataset size and generalization across geographies on 3D detection methods. Find data, code and more up-to-date information at http://www.waymo.com/open.},
  archiveprefix = {arXiv},
  eprint = {1912.04838},
  eprinttype = {arxiv},
  file = {/Users/anselm/Zotero/storage/Y5MWQQWK/Sun et al. - 2020 - Scalability in Perception for Autonomous Driving .pdf;/Users/anselm/Zotero/storage/INRT79FV/1912.html},
  journal = {arXiv:1912.04838 [cs, stat]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  primaryclass = {cs, stat}
}

@article{thrunProbabilisticRobotics2002,
  title = {Probabilistic Robotics},
  author = {Thrun, Sebastian},
  year = {2002},
  month = mar,
  volume = {45},
  pages = {52--57},
  issn = {0001-0782, 1557-7317},
  doi = {10.1145/504729.504754},
  file = {/Users/anselm/Zotero/storage/P3PPWNTL/Thrun - 2002 - Probabilistic robotics.pdf},
  journal = {Communications of the ACM},
  language = {en},
  number = {3}
}

@inproceedings{tsotsosRobustInferenceVisualinertial2015,
  title = {Robust Inference for Visual-Inertial Sensor Fusion},
  booktitle = {2015 {{IEEE International Conference}} on {{Robotics}} and {{Automation}} ({{ICRA}})},
  author = {Tsotsos, Konstantine and Chiuso, Alessandro and Soatto, Stefano},
  year = {2015},
  month = may,
  pages = {5203--5210},
  publisher = {{IEEE}},
  address = {{Seattle, WA, USA}},
  doi = {10.1109/ICRA.2015.7139924},
  file = {/Users/anselm/Zotero/storage/FY54AZWL/Tsotsos et al. - 2015 - Robust inference for visual-inertial sensor fusion.pdf},
  isbn = {978-1-4799-6923-4}
}

@article{usenkoDoubleSphereCamera2018,
  title = {The {{Double Sphere Camera Model}}},
  author = {Usenko, Vladyslav and Demmel, Nikolaus and Cremers, Daniel},
  year = {2018},
  month = sep,
  pages = {552--560},
  doi = {10.1109/3DV.2018.00069},
  abstract = {Vision-based motion estimation and 3D reconstruction, which have numerous applications (e.g., autonomous driving, navigation systems for airborne devices and augmented reality) are receiving significant research attention. To increase the accuracy and robustness, several researchers have recently demonstrated the benefit of using large fieldof-view cameras for such applications.},
  archiveprefix = {arXiv},
  eprint = {1807.08957},
  eprinttype = {arxiv},
  file = {/Users/anselm/Zotero/storage/HXJ5QFFN/Usenko et al. - 2018 - The Double Sphere Camera Model.pdf},
  journal = {2018 International Conference on 3D Vision (3DV)},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  language = {en}
}

@article{violaRobustRealTimeFace2004,
  title = {Robust {{Real}}-{{Time Face Detection}}},
  author = {Viola, Paul and Jones, Michael J.},
  year = {2004},
  month = may,
  volume = {57},
  pages = {137--154},
  issn = {1573-1405},
  doi = {10.1023/B:VISI.0000013087.49260.fb},
  abstract = {This paper describes a face detection framework that is capable of processing images extremely rapidly while achieving high detection rates. There are three key contributions. The first is the introduction of a new image representation called the ``Integral Image'' which allows the features used by our detector to be computed very quickly. The second is a simple and efficient classifier which is built using the AdaBoost learning algorithm (Freund and Schapire, 1995) to select a small number of critical visual features from a very large set of potential features. The third contribution is a method for combining classifiers in a ``cascade'' which allows background regions of the image to be quickly discarded while spending more computation on promising face-like regions. A set of experiments in the domain of face detection is presented. The system yields face detection performance comparable to the best previous systems (Sung and Poggio, 1998; Rowley et al., 1998; Schneiderman and Kanade, 2000; Roth et al., 2000). Implemented on a conventional desktop, face detection proceeds at 15 frames per second.},
  journal = {International Journal of Computer Vision},
  number = {2}
}

@article{violaRobustRealtimeObject,
  title = {Robust {{Real}}-Time {{Object Detection}}},
  author = {Viola, Paul and Jones, Michael},
  pages = {25},
  abstract = {This paper describes a visual object detection framework that is capable of processing images extremely rapidly while achieving high detection rates. There are three key contributions. The first is the introduction of a new image representation called the ``Integral Image'' which allows the features used by our detector to be computed very quickly. The second is a learning algorithm, based on AdaBoost, which selects a small number of critical visual features and yields extremely efficient classifiers [6]. The third contribution is a method for combining classifiers in a ``cascade'' which allows background regions of the image to be quickly discarded while spending more computation on promising object-like regions. A set of experiments in the domain of face detection are presented. The system yields face detection performace comparable to the best previous systems [18, 13, 16, 12, 1]. Implemented on a conventional desktop, face detection proceeds at 15 frames per second.},
  file = {/Users/anselm/Zotero/storage/7YAIH6SK/Viola and Jones - Robust Real-time Object Detection.pdf},
  language = {en}
}

@inproceedings{voigtlaenderMOTSMultiObjectTracking2019,
  title = {{{MOTS}}: {{Multi}}-{{Object Tracking}} and {{Segmentation}}},
  shorttitle = {{{MOTS}}},
  booktitle = {2019 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Voigtlaender, Paul and Krause, Michael and Osep, Aljosa and Luiten, Jonathon and Sekar, Berin Balachandar Gnana and Geiger, Andreas and Leibe, Bastian},
  year = {2019},
  month = jun,
  pages = {7934--7943},
  publisher = {{IEEE}},
  address = {{Long Beach, CA, USA}},
  doi = {10.1109/CVPR.2019.00813},
  abstract = {This paper extends the popular task of multi-object tracking to multi-object tracking and segmentation (MOTS). Towards this goal, we create dense pixel-level annotations for two existing tracking datasets using a semi-automatic annotation procedure. Our new annotations comprise 65,213 pixel masks for 977 distinct objects (cars and pedestrians) in 10,870 video frames. For evaluation, we extend existing multi-object tracking metrics to this new task. Moreover, we propose a new baseline method which jointly addresses detection, tracking, and segmentation with a single convolutional network. We demonstrate the value of our datasets by achieving improvements in performance when training on MOTS annotations. We believe that our datasets, metrics and baseline will become a valuable resource towards developing multi-object tracking approaches that go beyond 2D bounding boxes. We make our annotations, code, and models available at https: //www.vision.rwth-aachen.de/page/mots.},
  file = {/Users/anselm/Zotero/storage/5QMFV42Z/Voigtlaender et al. - 2019 - MOTS Multi-Object Tracking and Segmentation.pdf},
  isbn = {978-1-72813-293-8},
  language = {en}
}

@article{wangPseudoLiDARVisualDepth2018,
  title = {Pseudo-{{LiDAR}} from {{Visual Depth Estimation}}: {{Bridging}} the {{Gap}} in {{3D Object Detection}} for {{Autonomous Driving}}},
  shorttitle = {Pseudo-{{LiDAR}} from {{Visual Depth Estimation}}},
  author = {Wang, Yan and Chao, Wei-Lun and Garg, Divyansh and Hariharan, Bharath and Campbell, Mark and Weinberger, Kilian Q.},
  year = {2018},
  month = dec,
  abstract = {3D object detection is an essential task in autonomous driving. Recent techniques excel with highly accurate detection rates, provided the 3D input data is obtained from precise but expensive LiDAR technology. Approaches based on cheaper monocular or stereo imagery data have, until now, resulted in drastically lower accuracies --- a gap that is commonly attributed to poor image-based depth estimation. However, in this paper we argue that it is not the quality of the data but its representation that accounts for the majority of the difference. Taking the inner workings of convolutional neural networks into consideration, we propose to convert image-based depth maps to pseudo-LiDAR representations --- essentially mimicking the LiDAR signal. With this representation we can apply different existing LiDAR-based detection algorithms. On the popular KITTI benchmark, our approach achieves impressive improvements over the existing state-of-the-art in image-based performance --- raising the detection accuracy of objects within the 30m range from the previous state-of-the-art of 22\% to an unprecedented 74\%. At the time of submission our algorithm holds the highest entry on the KITTI 3D object detection leaderboard for stereo-image-based approaches. Our code is publicly available at https://github.com/mileyan/pseudo\_lidar.},
  file = {/Users/anselm/Zotero/storage/HTV4ZZCM/Wang et al. - 2018 - Pseudo-LiDAR from Visual Depth Estimation Bridgin.pdf;/Users/anselm/Zotero/storage/QCPD8M8E/1812.html},
  language = {en}
}

@article{wangRegionletsGenericObject,
  title = {Regionlets for {{Generic Object Detection}}},
  author = {Wang, Xiaoyu and Yang, Ming and Zhu, Shenghuo and Lin, Yuanqing},
  pages = {8},
  abstract = {Generic object detection is confronted by dealing with different degrees of variations in distinct object classes with tractable computations, which demands for descriptive and flexible object representations that are also efficient to evaluate for many locations. In view of this, we propose to model an object class by a cascaded boosting classifier which integrates various types of features from competing local regions, named as regionlets. A regionlet is a base feature extraction region defined proportionally to a detection window at an arbitrary resolution (i.e. size and aspect ratio). These regionlets are organized in small groups with stable relative positions to delineate fine-grained spatial layouts inside objects. Their features are aggregated to a one-dimensional feature within one group so as to tolerate deformations. Then we evaluate the object bounding box proposal in selective search from segmentation cues, limiting the evaluation locations to thousands. Our approach significantly outperforms the state-of-the-art on popular multi-class detection benchmark datasets with a single method, without any contexts. It achieves the detection mean average precision of 41.7\% on the PASCAL VOC 2007 dataset and 39.7\% on the VOC 2010 for 20 object categories, and 14.7\% mean average precision on the ImageNet dataset for 200 object categories, outperforming the latest deformable part-based model (DPM) by 4.7\%.},
  file = {/Users/anselm/Zotero/storage/P9Q87ZH9/Wang et al. - Regionlets for Generic Object Detection.pdf},
  language = {en}
}

@article{wangRegionletsGenericObject2015,
  title = {Regionlets for {{Generic Object Detection}}},
  author = {Wang, Xiaoyu and Yang, Ming and Zhu, Shenghuo and Lin, Yuanqing},
  year = {2015},
  month = oct,
  volume = {37},
  pages = {2071--2084},
  issn = {0162-8828, 2160-9292},
  doi = {10.1109/TPAMI.2015.2389830},
  file = {/Users/anselm/Zotero/storage/MT69GN7R/Wang et al. - 2015 - Regionlets for Generic Object Detection.pdf},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number = {10}
}

@article{wengBaseline3DMultiObject2019,
  title = {A {{Baseline}} for {{3D Multi}}-{{Object Tracking}}},
  author = {Weng, Xinshuo and Kitani, Kris},
  year = {2019},
  month = dec,
  abstract = {3D multi-object tracking (MOT) is an essential component technology for many real-time applications such as autonomous driving or assistive robotics. Recent work on 3D MOT tend to focus more on developing accurate systems giving less regard to computational cost and system complexity. In contrast, this work proposes a simple yet accurate real-time 3D MOT system. We use an off-the-shelf 3D object detector to obtain oriented 3D bounding boxes from the LiDAR point cloud. Then, a combination of 3D Kalman filter and Hungarian algorithm is used for state estimation and data association. Although our baseline system is a straightforward combination of standard methods, we obtain the state-of-the-art results. To evaluate our baseline system, we propose a new 3D MOT extension to the official KITTI 2D MOT evaluation along with a set of new metrics. Our proposed baseline method for 3D MOT establishes new state-of-the-art performance on 3D MOT for KITTI. Surprisingly, although our baseline system does not use any 2D data as input, we place 2nd on the official KITTI 2D MOT leaderboard. Also, our proposed 3D MOT method runs at a rate of \$214.7\$ FPS, achieving the fastest speed among all modern MOT systems. Our code is publicly available at https://github.com/xinshuoweng/AB3DMOT},
  archiveprefix = {arXiv},
  eprint = {1907.03961},
  eprinttype = {arxiv},
  file = {/Users/anselm/Zotero/storage/CI72L92P/Weng and Kitani - 2019 - A Baseline for 3D Multi-Object Tracking.pdf},
  journal = {arXiv:1907.03961 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Robotics},
  language = {en},
  primaryclass = {cs}
}

@article{wengMonocular3DObject2019,
  title = {Monocular {{3D Object Detection}} with {{Pseudo}}-{{LiDAR Point Cloud}}},
  author = {Weng, Xinshuo and Kitani, Kris},
  year = {2019},
  month = mar,
  abstract = {Monocular 3D scene understanding tasks, such as object size estimation, heading angle estimation and 3D localization, is challenging. Successful modern day methods for 3D scene understanding require the use of a 3D sensor. On the other hand, single image based methods have significantly worse performance. In this work, we aim at bridging the performance gap between 3D sensing and 2D sensing for 3D object detection by enhancing LiDAR-based algorithms to work with single image input. Specifically, we perform monocular depth estimation and lift the input image to a point cloud representation, which we call pseudo-LiDAR point cloud. Then we can train a LiDAR-based 3D detection network with our pseudo-LiDAR end-to-end. Following the pipeline of two-stage 3D detection algorithms, we detect 2D object proposals in the input image and extract a point cloud frustum from the pseudo-LiDAR for each proposal. Then an oriented 3D bounding box is detected for each frustum. To handle the large amount of noise in the pseudo-LiDAR, we propose two innovations: (1) use a 2D-3D bounding box consistency constraint, adjusting the predicted 3D bounding box to have a high overlap with its corresponding 2D proposal after projecting onto the image; (2) use the instance mask instead of the bounding box as the representation of 2D proposals, in order to reduce the number of points not belonging to the object in the point cloud frustum. Through our evaluation on the KITTI benchmark, we achieve the top-ranked performance on both bird's eye view and 3D object detection among all monocular methods, effectively quadrupling the performance over previous state-of-the-art. Our code is available at https://github.com/xinshuoweng/Mono3D\_PLiDAR.},
  file = {/Users/anselm/Zotero/storage/6ZS7LNUK/Weng and Kitani - 2019 - Monocular 3D Object Detection with Pseudo-LiDAR Po.pdf;/Users/anselm/Zotero/storage/5876YIJE/1903.html},
  language = {en}
}

@article{wengMonocular3DObject2019a,
  title = {Monocular {{3D Object Detection}} with {{Pseudo}}-{{LiDAR Point Cloud}}},
  author = {Weng, Xinshuo and Kitani, Kris},
  year = {2019},
  month = aug,
  abstract = {Monocular 3D scene understanding tasks, such as object size estimation, heading angle estimation and 3D localization, is challenging. Successful modern day methods for 3D scene understanding require the use of a 3D sensor. On the other hand, single image based methods have significantly worse performance. In this work, we aim at bridging the performance gap between 3D sensing and 2D sensing for 3D object detection by enhancing LiDAR-based algorithms to work with single image input. Specifically, we perform monocular depth estimation and lift the input image to a point cloud representation, which we call pseudo-LiDAR point cloud. Then we can train a LiDAR-based 3D detection network with our pseudo-LiDAR end-to-end. Following the pipeline of two-stage 3D detection algorithms, we detect 2D object proposals in the input image and extract a point cloud frustum from the pseudo-LiDAR for each proposal. Then an oriented 3D bounding box is detected for each frustum. To handle the large amount of noise in the pseudo-LiDAR, we propose two innovations: (1) use a 2D-3D bounding box consistency constraint, adjusting the predicted 3D bounding box to have a high overlap with its corresponding 2D proposal after projecting onto the image; (2) use the instance mask instead of the bounding box as the representation of 2D proposals, in order to reduce the number of points not belonging to the object in the point cloud frustum. Through our evaluation on the KITTI benchmark, we achieve the top-ranked performance on both bird's eye view and 3D object detection among all monocular methods, effectively quadrupling the performance over previous state-of-the-art. Our code is available at https://github.com/xinshuoweng/Mono3D\_PLiDAR.},
  archiveprefix = {arXiv},
  eprint = {1903.09847},
  eprinttype = {arxiv},
  file = {/Users/anselm/Zotero/storage/MK9Q2KW7/Weng and Kitani - 2019 - Monocular 3D Object Detection with Pseudo-LiDAR Po.pdf;/Users/anselm/Zotero/storage/4NR248N4/1903.html},
  journal = {arXiv:1903.09847 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  primaryclass = {cs}
}

@article{xiangSubcategoryawareConvolutionalNeural2017,
  title = {Subcategory-Aware {{Convolutional Neural Networks}} for {{Object Proposals}} and {{Detection}}},
  author = {Xiang, Yu and Choi, Wongun and Lin, Yuanqing and Savarese, Silvio},
  year = {2017},
  month = mar,
  abstract = {In CNN-based object detection methods, region proposal becomes a bottleneck when objects exhibit significant scale variation, occlusion or truncation. In addition, these methods mainly focus on 2D object detection and cannot estimate detailed properties of objects. In this paper, we propose subcategory-aware CNNs for object detection. We introduce a novel region proposal network that uses subcategory information to guide the proposal generating process, and a new detection network for joint detection and subcategory classification. By using subcategories related to object pose, we achieve state-of-the-art performance on both detection and pose estimation on commonly used benchmarks.},
  archiveprefix = {arXiv},
  eprint = {1604.04693},
  eprinttype = {arxiv},
  file = {/Users/anselm/Zotero/storage/IC7Q24IZ/Xiang et al. - 2017 - Subcategory-aware Convolutional Neural Networks fo.pdf;/Users/anselm/Zotero/storage/4JI8KXEP/1604.html},
  journal = {arXiv:1604.04693 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  primaryclass = {cs}
}

@article{xiao-shangaoCompleteSolutionClassification2003,
  title = {Complete Solution Classification for the Perspective-Three-Point Problem},
  author = {{Xiao-Shan Gao} and {Xiao-Rong Hou} and {Jianliang Tang} and {Hang-Fei Cheng}},
  year = {2003},
  month = aug,
  volume = {25},
  pages = {930--943},
  issn = {0162-8828},
  doi = {10.1109/TPAMI.2003.1217599},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  language = {en},
  number = {8}
}

@article{yangCubeSLAMMonocular3D2019,
  title = {{{CubeSLAM}}: {{Monocular 3D Object SLAM}}},
  shorttitle = {{{CubeSLAM}}},
  author = {Yang, Shichao and Scherer, Sebastian},
  year = {2019},
  month = aug,
  volume = {35},
  pages = {925--938},
  issn = {1552-3098, 1941-0468},
  doi = {10.1109/TRO.2019.2909168},
  abstract = {We present a method for single image 3D cuboid object detection and multi-view object SLAM in both static and dynamic environments, and demonstrate that the two parts can improve each other. Firstly for single image object detection, we generate high-quality cuboid proposals from 2D bounding boxes and vanishing points sampling. The proposals are further scored and selected based on the alignment with image edges. Secondly, multi-view bundle adjustment with new object measurements is proposed to jointly optimize poses of cameras, objects and points. Objects can provide long-range geometric and scale constraints to improve camera pose estimation and reduce monocular drift. Instead of treating dynamic regions as outliers, we utilize object representation and motion model constraints to improve the camera pose estimation. The 3D detection experiments on SUN RGBD and KITTI show better accuracy and robustness over existing approaches. On the public TUM, KITTI odometry and our own collected datasets, our SLAM method achieves the stateof-the-art monocular camera pose estimation and at the same time, improves the 3D object detection accuracy.},
  archiveprefix = {arXiv},
  eprint = {1806.00557},
  eprinttype = {arxiv},
  file = {/Users/anselm/Zotero/storage/VLKJQMNN/Yang and Scherer - 2019 - CubeSLAM Monocular 3D Object SLAM.pdf},
  journal = {IEEE Transactions on Robotics},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Robotics},
  language = {en},
  number = {4}
}

@article{yangVideoInstanceSegmentation2019,
  title = {Video {{Instance Segmentation}}},
  author = {Yang, Linjie and Fan, Yuchen and Xu, Ning},
  year = {2019},
  month = aug,
  abstract = {In this paper we present a new computer vision task, named video instance segmentation. The goal of this new task is simultaneous detection, segmentation and tracking of instances in videos. In words, it is the first time that the image instance segmentation problem is extended to the video domain. To facilitate research on this new task, we propose a large-scale benchmark called YouTube-VIS, which consists of 2883 high-resolution YouTube videos, a 40-category label set and 131k high-quality instance masks. In addition, we propose a novel algorithm called MaskTrack R-CNN for this task. Our new method introduces a new tracking branch to Mask R-CNN to jointly perform the detection, segmentation and tracking tasks simultaneously. Finally, we evaluate the proposed method and several strong baselines on our new dataset. Experimental results clearly demonstrate the advantages of the proposed algorithm and reveal insight for future improvement. We believe the video instance segmentation task will motivate the community along the line of research for video understanding.},
  archiveprefix = {arXiv},
  eprint = {1905.04804},
  eprinttype = {arxiv},
  file = {/Users/anselm/Zotero/storage/W8UBGGZ2/Yang et al. - 2019 - Video Instance Segmentation.pdf;/Users/anselm/Zotero/storage/5TLG7PP4/1905.html},
  journal = {arXiv:1905.04804 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  primaryclass = {cs}
}

@incollection{yiLIFTLearnedInvariant2016,
  title = {{{LIFT}}: {{Learned Invariant Feature Transform}}},
  shorttitle = {{{LIFT}}},
  booktitle = {Computer {{Vision}} \textendash{} {{ECCV}} 2016},
  author = {Yi, Kwang Moo and Trulls, Eduard and Lepetit, Vincent and Fua, Pascal},
  editor = {Leibe, Bastian and Matas, Jiri and Sebe, Nicu and Welling, Max},
  year = {2016},
  volume = {9910},
  pages = {467--483},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-319-46466-4_28},
  abstract = {We introduce a novel Deep Network architecture that implements the full feature point handling pipeline, that is, detection, orientation estimation, and feature description. While previous works have successfully tackled each one of these problems individually, we show how to learn to do all three in a unified manner while preserving end-to-end differentiability. We then demonstrate that our Deep pipeline outperforms state-of-the-art methods on a number of benchmark datasets, without the need of retraining.},
  file = {/Users/anselm/Zotero/storage/9BKCRFXL/Yi et al. - 2016 - LIFT Learned Invariant Feature Transform.pdf},
  isbn = {978-3-319-46465-7 978-3-319-46466-4},
  language = {en}
}

@article{yinCenterbased3DObject2021,
  title = {Center-Based {{3D Object Detection}} and {{Tracking}}},
  author = {Yin, Tianwei and Zhou, Xingyi and Kr{\"a}henb{\"u}hl, Philipp},
  year = {2021},
  month = jan,
  abstract = {Three-dimensional objects are commonly represented as 3D boxes in a point-cloud. This representation mimics the well-studied image-based 2D bounding-box detection but comes with additional challenges. Objects in a 3D world do not follow any particular orientation, and box-based detectors have difficulties enumerating all orientations or fitting an axis-aligned bounding box to rotated objects. In this paper, we instead propose to represent, detect, and track 3D objects as points. Our framework, CenterPoint, first detects centers of objects using a keypoint detector and regresses to other attributes, including 3D size, 3D orientation, and velocity. In a second stage, it refines these estimates using additional point features on the object. In CenterPoint, 3D object tracking simplifies to greedy closest-point matching. The resulting detection and tracking algorithm is simple, efficient, and effective. CenterPoint achieved state-of-theart performance on the nuScenes benchmark for both 3D detection and tracking, with 65.5 NDS and 63.8 AMOTA for a single model. On the Waymo Open Dataset, CenterPoint outperforms all previous single model method by a large margin and ranks first among all Lidar-only submissions. The code and pretrained models are available at https://github.com/tianweiy/CenterPoint.},
  archiveprefix = {arXiv},
  eprint = {2006.11275},
  eprinttype = {arxiv},
  file = {/Users/anselm/Zotero/storage/V3PY9PE4/Yin et al. - 2021 - Center-based 3D Object Detection and Tracking.pdf},
  journal = {arXiv:2006.11275 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  language = {en},
  primaryclass = {cs}
}

@inproceedings{youPseudoLiDARAccurateDepth2019,
  title = {Pseudo-{{LiDAR}}++: {{Accurate Depth}} for {{3D Object Detection}} in {{Autonomous Driving}}},
  shorttitle = {Pseudo-{{LiDAR}}++},
  booktitle = {International {{Conference}} on {{Learning Representations}}},
  author = {You, Yurong and Wang, Yan and Chao, Wei-Lun and Garg, Divyansh and Pleiss, Geoff and Hariharan, Bharath and Campbell, Mark and Weinberger, Kilian Q.},
  year = {2019},
  month = sep,
  abstract = {Detecting objects such as cars and pedestrians in 3D plays an indispensable role in autonomous driving. Existing approaches largely rely on expensive LiDAR sensors for accurate depth information....},
  file = {/Users/anselm/Zotero/storage/QLFXDD5P/You et al. - 2019 - Pseudo-LiDAR++ Accurate Depth for 3D Object Detec.pdf;/Users/anselm/Zotero/storage/PRJIT9VY/forum.html}
}

@inproceedings{yuDSSLAMSemanticVisual2018,
  title = {{{DS}}-{{SLAM}}: {{A Semantic Visual SLAM}} towards {{Dynamic Environments}}},
  shorttitle = {{{DS}}-{{SLAM}}},
  booktitle = {2018 {{IEEE}}/{{RSJ International Conference}} on {{Intelligent Robots}} and {{Systems}} ({{IROS}})},
  author = {Yu, Chao and Liu, Zuxin and Liu, Xin-Jun and Xie, Fugui and Yang, Yi and Wei, Qi and Fei, Qiao},
  year = {2018},
  month = oct,
  pages = {1168--1174},
  publisher = {{IEEE}},
  address = {{Madrid}},
  doi = {10.1109/IROS.2018.8593691},
  abstract = {Simultaneous Localization and Mapping (SLAM) is considered to be a fundamental capability for intelligent mobile robots. Over the past decades, many impressed SLAM systems have been developed and achieved good performance under certain circumstances. However, some problems are still not well solved, for example, how to tackle the moving objects in the dynamic environments, how to make the robots truly understand the surroundings and accomplish advanced tasks. In this paper, a robust semantic visual SLAM towards dynamic environments named DS-SLAM is proposed. Five threads run in parallel in DS-SLAM: tracking, semantic segmentation, local mapping, loop closing and dense semantic map creation. DS-SLAM combines semantic segmentation network with moving consistency check method to reduce the impact of dynamic objects, and thus the localization accuracy is highly improved in dynamic environments. Meanwhile, a dense semantic octo-tree map is produced, which could be employed for high-level tasks. We conduct experiments both on TUM RGB-D dataset and in real-world environment. The results demonstrate the absolute trajectory accuracy in DS-SLAM can be improved one order of magnitude compared with ORB-SLAM2. It is one of the state-of-the-art SLAM systems in high-dynamic environments.},
  file = {/Users/anselm/Zotero/storage/HR4CR4MA/Yu et al. - 2018 - DS-SLAM A Semantic Visual SLAM towards Dynamic En.pdf},
  isbn = {978-1-5386-8094-0},
  language = {en}
}

@article{zhangFlexibleNewTechnique2000,
  title = {A Flexible New Technique for Camera Calibration},
  author = {Zhang, Z.},
  year = {Nov./2000},
  volume = {22},
  pages = {1330--1334},
  issn = {01628828},
  doi = {10.1109/34.888718},
  abstract = {We propose a flexible new technique to easily calibrate a camera. It is well suited for use without specialized knowledge of 3D geometry or computer vision. The technique only requires the camera to observe a planar pattern shown at a few (at least two) different orientations. Either the camera or the planar pattern can be freely moved. The motion need not be known. Radial lens distortion is modeled. The proposed procedure consists of a closed-form solution, followed by a nonlinear refinement based on the maximum likelihood criterion. Both computer simulation and real data have been used to test the proposed technique, and very good results have been obtained. Compared with classical techniques which use expensive equipment such as two or three orthogonal planes, the proposed technique is easy to use and flexible. It advances 3D computer vision one step from laboratory environments to real world use.},
  file = {/Users/anselm/Zotero/storage/SXNYUHQR/Zhang - 2000 - A flexible new technique for camera calibration.pdf},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  language = {en},
  number = {11}
}

@article{zhangUnderstandingHighLevelSemantics,
  title = {Understanding {{High}}-{{Level Semantics}} by {{Modeling Traffic Patterns}}},
  author = {Zhang, Hongyi and Geiger, Andreas and Urtasun, Raquel},
  pages = {8},
  abstract = {In this paper, we are interested in understanding the semantics of outdoor scenes in the context of autonomous driving. Towards this goal, we propose a generative model of 3D urban scenes which is able to reason not only about the geometry and objects present in the scene, but also about the high-level semantics in the form of traffic patterns. We found that a small number of patterns is sufficient to model the vast majority of traffic scenes and show how these patterns can be learned. As evidenced by our experiments, this high-level reasoning significantly improves the overall scene estimation as well as the vehicle-to-lane association when compared to state-of-the-art approaches [10].},
  file = {/Users/anselm/Zotero/storage/K9J8938V/Zhang et al. - Understanding High-Level Semantics by Modeling Tra.pdf},
  language = {en}
}

@inproceedings{zhangUnderstandingHighLevelSemantics2013,
  title = {Understanding {{High}}-{{Level Semantics}} by {{Modeling Traffic Patterns}}},
  booktitle = {2013 {{IEEE International Conference}} on {{Computer Vision}}},
  author = {Zhang, Hongyi and Geiger, Andreas and Urtasun, Raquel},
  year = {2013},
  month = dec,
  pages = {3056--3063},
  publisher = {{IEEE}},
  address = {{Sydney, Australia}},
  doi = {10.1109/ICCV.2013.379},
  file = {/Users/anselm/Zotero/storage/G57X9L25/Zhang et al. - 2013 - Understanding High-Level Semantics by Modeling Tra.pdf},
  isbn = {978-1-4799-2840-8}
}

@article{zhouObjectsPoints2019,
  title = {Objects as {{Points}}},
  author = {Zhou, Xingyi and Wang, Dequan and Kr{\"a}henb{\"u}hl, Philipp},
  year = {2019},
  month = apr,
  abstract = {Detection identifies objects as axis-aligned boxes in an image. Most successful object detectors enumerate a nearly exhaustive list of potential object locations and classify each. This is wasteful, inefficient, and requires additional post-processing. In this paper, we take a different approach. We model an object as a single point --- the center point of its bounding box. Our detector uses keypoint estimation to find center points and regresses to all other object properties, such as size, 3D location, orientation, and even pose. Our center point based approach, CenterNet, is end-to-end differentiable, simpler, faster, and more accurate than corresponding bounding box based detectors. CenterNet achieves the best speed-accuracy trade-off on the MS COCO dataset, with 28.1\% AP at 142 FPS, 37.4\% AP at 52 FPS, and 45.1\% AP with multi-scale testing at 1.4 FPS. We use the same approach to estimate 3D bounding box in the KITTI benchmark and human pose on the COCO keypoint dataset. Our method performs competitively with sophisticated multi-stage methods and runs in real-time.},
  archiveprefix = {arXiv},
  eprint = {1904.07850},
  eprinttype = {arxiv},
  file = {/Users/anselm/Zotero/storage/RQ5FKYMJ/Zhou et al. - 2019 - Objects as Points.pdf;/Users/anselm/Zotero/storage/AWXCFPM5/1904.html},
  journal = {arXiv:1904.07850 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  primaryclass = {cs}
}

@article{zhouOpen3DModernLibrary2018,
  title = {{{Open3D}}: {{A Modern Library}} for {{3D Data Processing}}},
  shorttitle = {{{Open3D}}},
  author = {Zhou, Qian-Yi and Park, Jaesik and Koltun, Vladlen},
  year = {2018},
  month = jan,
  abstract = {Open3D is an open-source library that supports rapid development of software that deals with 3D data. The Open3D frontend exposes a set of carefully selected data structures and algorithms in both C++ and Python. The backend is highly optimized and is set up for parallelization. Open3D was developed from a clean slate with a small and carefully considered set of dependencies. It can be set up on different platforms and compiled from source with minimal effort. The code is clean, consistently styled, and maintained via a clear code review mechanism. Open3D has been used in a number of published research projects and is actively deployed in the cloud. We welcome contributions from the open-source community.},
  archiveprefix = {arXiv},
  eprint = {1801.09847},
  eprinttype = {arxiv},
  file = {/Users/anselm/Zotero/storage/CFRQKFXT/Zhou et al. - 2018 - Open3D A Modern Library for 3D Data Processing.pdf;/Users/anselm/Zotero/storage/BW85FZS4/1801.html},
  journal = {arXiv:1801.09847 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Graphics,Computer Science - Robotics},
  primaryclass = {cs}
}


